{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#  Text to Word : Split\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Length of the senetence\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length of the word in document\n",
    "np.max([len(x) for x in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'is', 'brown', 'the', 'he', 'over', 'jumping', 'lazy', 'dog', 'and', 'quick', 'fox'}\n",
      "Vocabulary size: 11\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary List\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "vocab = set(text_to_word_sequence(sentence))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary:\",vocab)\n",
    "print(\"Vocabulary size:\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'james', 'bible'],\n",
       " ['old', 'testament', 'king', 'james', 'bible'],\n",
       " ['first', 'book', 'moses', 'called', 'genesis'],\n",
       " ['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters'],\n",
       " ['god', 'said', 'let', 'light', 'light'],\n",
       " ['god', 'saw', 'light', 'good', 'god', 'divided', 'light', 'darkness'],\n",
       " ['god', 'called', 'light', 'day', 'darkness', 'called', 'night'],\n",
       " ['evening', 'morning', 'first', 'day']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to word Sequence\n",
    "\n",
    "from keras.preprocessing.text import text\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "[[w for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'james', 'bible'],\n",
       " ['old', 'testament', 'king', 'james', 'bible'],\n",
       " ['first', 'book', 'moses', 'called', 'genesis'],\n",
       " ['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters'],\n",
       " ['god', 'said', 'let', 'light', 'light'],\n",
       " ['god', 'saw', 'light', 'good', 'god', 'divided', 'light', 'darkness'],\n",
       " ['god', 'called', 'light', 'day', 'darkness', 'called', 'night'],\n",
       " ['evening', 'morning', 'first', 'day']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using List comprehension\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "[[word for word in document.lower().split()] for document in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'brown': 1, 'fox': 1, 'is': 2, 'quick': 1, 'and': 1, 'he': 1, 'jumping': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# Word Frequency (Word Count)\n",
    "\n",
    "word_freq = {}\n",
    "\n",
    "for tok in sentence.split():\n",
    "    if tok in word_freq:\n",
    "        word_freq[tok] +=1\n",
    "    else:\n",
    "        word_freq[tok] = 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 2, 'The': 1, 'brown': 1, 'fox': 1, 'quick': 1, 'and': 1, 'he': 1, 'jumping': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1})\n"
     ]
    }
   ],
   "source": [
    "# Word Frequency (Word Count)\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(sentence.split())\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 2),\n",
       " ('The', 1),\n",
       " ('brown', 1),\n",
       " ('fox', 1),\n",
       " ('quick', 1),\n",
       " ('and', 1),\n",
       " ('he', 1),\n",
       " ('jumping', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Most Common word\n",
    "counter.most_common(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'brown': 1, 'The': 2, 'the': 3, 'he': 4, 'over': 5, 'jumping': 6, 'lazy': 7, 'dog': 8, 'and': 9, 'quick': 10, 'fox': 11}\n"
     ]
    }
   ],
   "source": [
    "# Word Index\n",
    "\n",
    "corpus = sentence.split()\n",
    "\n",
    "uniq_text = set(corpus)\n",
    "\n",
    "text_to_int = {}\n",
    "\n",
    "for i, c in enumerate (uniq_text):\n",
    "    text_to_int.update({c: i})\n",
    "    \n",
    "print(text_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'brown', 'the', 'he', 'over', 'jumping', 'lazy', 'dog', 'and', 'quick', 'fox'}\n"
     ]
    }
   ],
   "source": [
    "# Unique words in Senetnce\n",
    "\n",
    "print(set(sentence.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'brown'],\n",
       " ['brown', 'fox'],\n",
       " ['fox', 'is'],\n",
       " ['is', 'quick'],\n",
       " ['quick', 'and'],\n",
       " ['and', 'he'],\n",
       " ['he', 'is'],\n",
       " ['is', 'jumping'],\n",
       " ['jumping', 'over'],\n",
       " ['over', 'the'],\n",
       " ['the', 'lazy'],\n",
       " ['lazy', 'dog']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram (n =2): Bigram\n",
    "\n",
    "words =sentence.split()\n",
    "\n",
    "n = 2\n",
    "output= []\n",
    "for i in range(len(words) - n+1):\n",
    "    output.append(words[i:i+n])\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'brown', 'fox'],\n",
       " ['brown', 'fox', 'is'],\n",
       " ['fox', 'is', 'quick'],\n",
       " ['is', 'quick', 'and'],\n",
       " ['quick', 'and', 'he'],\n",
       " ['and', 'he', 'is'],\n",
       " ['he', 'is', 'jumping'],\n",
       " ['is', 'jumping', 'over'],\n",
       " ['jumping', 'over', 'the'],\n",
       " ['over', 'the', 'lazy'],\n",
       " ['the', 'lazy', 'dog']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram (n =3): Trigram\n",
    "\n",
    "words =sentence.split()\n",
    "\n",
    "n = 3\n",
    "output= []\n",
    "for i in range(len(words) - n+1):\n",
    "    output.append(words[i:i+n])\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Located': 1,\n",
       " 'on': 2,\n",
       " 'the': 9,\n",
       " 'southern': 4,\n",
       " 'tip': 5,\n",
       " 'of': 6,\n",
       " 'Lake': 7,\n",
       " 'Union,': 8,\n",
       " 'Hilton': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to Integer\n",
    "\n",
    "list1 = ['Located', 'on', 'the', 'southern', 'tip', 'of', 'Lake', 'Union,', 'the', 'Hilton']\n",
    "\n",
    "text_int ={}\n",
    "\n",
    "for i,j in enumerate(list1):\n",
    "    text_int.update({j:(i+1)})\n",
    "    \n",
    "text_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6, 7],\n",
       " [13, 14, 5, 6, 7],\n",
       " [8, 15, 16, 3, 17],\n",
       " [18, 1, 19, 20, 9],\n",
       " [9, 21, 22, 23, 4, 10, 11, 24],\n",
       " [25, 1, 26, 10, 11, 27],\n",
       " [1, 28, 29, 2, 2],\n",
       " [1, 30, 2, 31, 1, 32, 2, 4],\n",
       " [1, 3, 2, 12, 4, 3, 33],\n",
       " [34, 35, 8, 12]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to Integer\n",
    "\n",
    "from keras.preprocessing import text\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary One Hot Encoder for One Senetence\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Bag of Word\n",
    "\n",
    "# One Hot Encoding\n",
    "\n",
    "doc = \"Can I eat the Pizza\".lower().split()\n",
    "\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(doc)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(doc, mode='count')\n",
    "\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Ending for Multiple Sentence\n",
    "\n",
    "doc = [['can', 'i', 'eat', 'the', 'pizza'],['can', 'i', 'eat', 'the', 'pizza']]\n",
    "\n",
    "\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(doc)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(doc, mode='count')\n",
    "\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding : Binary\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# define 5 documents\n",
    "\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!']\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "\n",
    "print(encoded_docs)\n",
    "\n",
    "# Word Index\n",
    "\n",
    "#'work': 1,\n",
    "# 'well': 2,\n",
    "# 'done': 3,\n",
    "# 'good': 4,\n",
    "# 'great': 5,\n",
    "# 'effort': 6,\n",
    "# 'nice': 7,\n",
    "# 'excellent': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi Dimenstional One Hot Encosing\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x = [[1,2,3,4,5],\n",
    "     [1,4,8,4,4],\n",
    "     [1,2,3,3,5]]\n",
    "\n",
    "y = to_categorical(x, num_classes=10)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input Array\n",
      "[[1, 2, 3, 4, 5], [1, 4, 8, 4, 4], [1, 2, 3, 3, 5]]\n",
      "\n",
      "\n",
      "One Hot Encoding of input:\n",
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]]\n",
      "\n",
      "\n",
      "Decoded output of One Hot Encoding\n",
      "[[1, 2, 3, 4, 5], [1, 4, 8, 4, 4], [1, 2, 3, 3, 5]]\n"
     ]
    }
   ],
   "source": [
    "# Decode One Hot Encoded Array\n",
    "\n",
    "from numpy import argmax\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x = [[1,2,3,4,5],\n",
    "     [1,4,8,4,4],\n",
    "     [1,2,3,3,5]]\n",
    "\n",
    "print(\"input Array\")\n",
    "print(x)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "one_hot_arry = to_categorical(x, num_classes=10)\n",
    "\n",
    "print(\"One Hot Encoding of input:\")\n",
    "print(one_hot_arry)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Decoded output of One Hot Encoding\")\n",
    "decoded_one_hot = [[argmax(aa) for aa in array] for array in one_hot_arry]\n",
    "\n",
    "print(decoded_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 6)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion of Dimention of data : specially for LSTM\n",
    "\n",
    "# Ex : list to array of (6,) to (1,1,6)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "one_hot1 = [[0,0,0,1,0,0],\n",
    "           [0,0,0,1,0,1],\n",
    "           [0,1,0,1,0,0]]# This is List of list type\n",
    "\n",
    "one_hot2 = [[0,0,0,1,0,0],\n",
    "           [0,0,0,1,0,1],\n",
    "           [0,1,0,1,0,0]]# This is List of list type\n",
    "\n",
    "# convert to format it can be used in the LSTM input\n",
    "\n",
    "liss = [one_hot1,one_hot2]\n",
    "\n",
    "np.asarray(liss).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding of the Characters in the string : for LSTM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "word_vec_length = 10 # maximum length of the name\n",
    "\n",
    "char_to_int = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, \n",
    "               'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, \n",
    "               'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, \n",
    "               'w': 22, 'x': 23, 'y': 24, 'z': 25, 'ö': 26, 'ä': 27, 'ü': 28,\n",
    "               '-': 29, 'ß': 30, ' ': 31}\n",
    "\n",
    "char_vec_length = len(char_to_int)\n",
    "\n",
    "\n",
    "# Returns a list of n lists with n = word_vec_length\n",
    "def name_encoding(name):\n",
    "\n",
    "    # Encode input data to int, e.g. a->1, z->26\n",
    "    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i < word_vec_length]\n",
    "    \n",
    "    # Start one-hot-encoding\n",
    "    onehot_encoded = list()\n",
    "    \n",
    "    for value in integer_encoded:\n",
    "        # create a list of n zeros, where n is equal to the number of accepted characters\n",
    "        letter = [0 for _ in range(char_vec_length)]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "        \n",
    "    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array\n",
    "    for _ in range(word_vec_length - len(name)):\n",
    "        onehot_encoded.append([0 for _ in range(char_vec_length)])\n",
    "        \n",
    "    return onehot_encoded\n",
    "\n",
    "names = ['santhosh','anand','sachin','prashanth']\n",
    "\n",
    "# use asarray to use change the shape of the array which suits for LSTM\n",
    "enc = np.asarray([np.asarray(name_encoding(name)) for name in names])\n",
    "\n",
    "print(enc.shape)\n",
    "\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"santhosh\",\"kumar\",\"anand\",\"joshi\"] # Samples\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# get the alphabest\n",
    "string.ascii_lowercase\n",
    "\n",
    "# List of Vocabulary\n",
    "vocabs_ = [x for x in string.ascii_lowercase] # Features\n",
    "\n",
    "# vocabulary length\n",
    "vocab_length = len(string.ascii_lowercase)\n",
    "\n",
    "# Max Sequence Length\n",
    "max_seq_length = max([len(x) for x in names])\n",
    "\n",
    "char_2_int = {} \n",
    "int_2_char = {}\n",
    "\n",
    "# Char to Interger \n",
    "char_2_int = dict([(x,i) for i,x in enumerate(vocabs_)])\n",
    "\n",
    "# Integer to Char\n",
    "int_2_char = dict([(i,x) for i,x in enumerate(vocabs_)])\n",
    "\n",
    "# Create Zero array for encoding\n",
    "\n",
    "encoding = np.zeros((len(names),max_seq_length,vocab_length))\n",
    "\n",
    "print(encoding.shape)  # (4 Samples,max length of samples,vocabulary Length\n",
    "\n",
    "# Update the encoder matrix for the samples\n",
    "\n",
    "for i,name in enumerate(names): # i = index value and i = sample and j =time step\n",
    "    for j,char in enumerate(name):\n",
    "        encoding[i,j,char_2_int[char]] = 1.0\n",
    "        \n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "doc =  ['S','C','Q','S','C','Q','S','C','Q','Q','C','Q','C','Q','Q']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(['S','C','Q'])\n",
    "\n",
    "label_encoder.transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OrdinalEncoder converts each string value to a whole number. \n",
    "# The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on.\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "doc =  [['S'],['C'],['Q'],['S'],['C'],['Q'],['S'],['C'],['Q'],['Q'],['C'],['Q'],['C'],['Q'],['Q']]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "encoder.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [2., 3.],\n",
       "       [1., 1.],\n",
       "       [0., 2.],\n",
       "       [2., 3.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OrdinalEncoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[\"good\", \"london\"],\n",
    "             [\"good\", \"tokyo\"],\n",
    "             [\"bad\", \"paris\"],\n",
    "             [\"average\", \"so so\"],\n",
    "             [\"good\", \"tokyo\"]])\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "encoder.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "bag.toarray()\n",
    "\n",
    "# Index\n",
    "\n",
    "# and = 0\n",
    "# is =1\n",
    "# shining = 2\n",
    "# sun =3\n",
    "# sweet =4\n",
    "# the =5\n",
    "# wether =6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>arts</th>\n",
       "      <th>at</th>\n",
       "      <th>becomes</th>\n",
       "      <th>between</th>\n",
       "      <th>both</th>\n",
       "      <th>brained</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>left</th>\n",
       "      <th>natural</th>\n",
       "      <th>of</th>\n",
       "      <th>overlap</th>\n",
       "      <th>part</th>\n",
       "      <th>processing</th>\n",
       "      <th>right</th>\n",
       "      <th>science</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.257439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420947</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249322</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183502</td>\n",
       "      <td>0.351643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         an       and       are      arts        at   becomes   between  \\\n",
       "0  0.403328  0.257439  0.000000  0.257439  0.000000  0.000000  0.403328   \n",
       "1  0.000000  0.159139  0.498644  0.159139  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.224449  0.000000  0.224449  0.351643  0.351643  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       both   brained      data  ...  language      left   natural        of  \\\n",
       "0  0.000000  0.000000  0.317989  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.498644  0.000000  ...  0.000000  0.249322  0.000000  0.000000   \n",
       "2  0.351643  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.308872  ...  0.391765  0.000000  0.391765  0.391765   \n",
       "\n",
       "    overlap      part  processing     right   science      time  \n",
       "0  0.403328  0.000000    0.000000  0.000000  0.420947  0.000000  \n",
       "1  0.000000  0.000000    0.000000  0.249322  0.130107  0.000000  \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.183502  0.351643  \n",
       "3  0.000000  0.391765    0.391765  0.000000  0.204439  0.000000  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF\n",
    "\n",
    "# tf-idf = tf(i,d) * idf(t,d)\n",
    "\n",
    "# td(i,d) = term frequency\n",
    "\n",
    "# idf(t,d) = \tlog(n/1+df(t,d)) :  inverse document frequency  df(t,d) : no. of documents d that contain the term t\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"Data Science is an overlap between Arts and Science\",\n",
    "          \"Generally,Arts graduates are right-brained and Science graduates are left-brained\",\n",
    "          \"Excelling in both Arts and Science at a time becomes difficult\",\n",
    "          \"Natural Language Processing is a part of Data Science\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "features = tfidf_model.fit_transform(corpus)\n",
    "df = pd.DataFrame(features.todense(),columns= sorted(tfidf_model.vocabulary_))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Idf\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"C:\\MyWork\\MyLearning\\Career Growth\\ML\\Files\\DataSet\\Consumer Complaint.csv\")\n",
    "\n",
    "data = data[['Product','Issue']]\n",
    "\n",
    "# Remove the null Values\n",
    "data = data.dropna(subset=['Issue'])\n",
    "\n",
    "data['category_id'] = data['Product'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Communication tactics</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Application, originator, mortgage broker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Other</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Product                                     Issue  category_id\n",
       "0  Debt collection                     Communication tactics            0\n",
       "1  Debt collection     Cont'd attempts collect debt not owed            0\n",
       "2         Mortgage  Application, originator, mortgage broker            1\n",
       "3      Credit card                                     Other            2\n",
       "4  Debt collection     Cont'd attempts collect debt not owed            0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Consumer loan</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Payday loan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>Money transfers</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>Student loan</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>Prepaid card</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>Other financial service</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Product  category_id\n",
       "0             Debt collection            0\n",
       "2                    Mortgage            1\n",
       "3                 Credit card            2\n",
       "6               Consumer loan            3\n",
       "11    Bank account or service            4\n",
       "13                Payday loan            5\n",
       "26           Credit reporting            6\n",
       "39            Money transfers            7\n",
       "103              Student loan            8\n",
       "730              Prepaid card            9\n",
       "1134  Other financial service           10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_id_df = data[['Product', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "\n",
    "category_id_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Debt collection',\n",
       " 1: 'Mortgage',\n",
       " 2: 'Credit card',\n",
       " 3: 'Consumer loan',\n",
       " 4: 'Bank account or service',\n",
       " 5: 'Payday loan',\n",
       " 6: 'Credit reporting',\n",
       " 7: 'Money transfers',\n",
       " 8: 'Student loan',\n",
       " 9: 'Prepaid card',\n",
       " 10: 'Other financial service'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_to_id = dict(category_id_df.values)\n",
    "\n",
    "id_to_category = dict(category_id_df[['category_id', 'Product']].values)\n",
    "\n",
    "id_to_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28154, 304)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# sublinear_df is set to True to use a logarithmic form for frequency.\n",
    "\n",
    "# min_df is the minimum numbers of documents a word must be present in to be kept\n",
    "\n",
    "# norm is set to l2, to ensure all our feature vectors have a euclidian norm of 1.\n",
    "\n",
    "# ngram_range is set to (1, 2) to indicate that we want to consider both unigrams and bigrams.\n",
    "\n",
    "# stop_words is set to \"english\" to remove all common pronouns (\"a\", \"the\", ...) to reduce the number of noisy features.\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(data.Issue).toarray()\n",
    "\n",
    "labels = data.Product\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Bank account or service':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Consumer loan':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Credit card':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Credit reporting':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Debt collection':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Money transfers':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Mortgage':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Other financial service':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Payday loan':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Prepaid card':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n",
      "# 'Student loan':\n",
      "  . Most correlated unigrams:\n",
      ". decision\n",
      ". deposits\n",
      "  . Most correlated bigrams:\n",
      ". debt protection\n",
      ". wrong day\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "N = 2\n",
    "\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    \n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    \n",
    "    print(\"# '{}':\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence from List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,   91,    1,  645, 1253,  927],\n",
       "       [   0,   73,    8, 3215,   55,  927],\n",
       "       [   0,    0,    0,  711,  632,   71]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad Sequence (Pre)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "raw_inputs = [[83, 91, 1, 645, 1253, 927],\n",
    "              [73, 8, 3215, 55, 927],\n",
    "              [711, 632, 71]]\n",
    "\n",
    "padded_inputs = pad_sequences(raw_inputs,padding='pre')\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,   91,    1,  645, 1253,  927],\n",
       "       [  73,    8, 3215,   55,  927,    0],\n",
       "       [ 711,  632,   71,    0,    0,    0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad Sequence (Post)\n",
    "\n",
    "raw_inputs = [[83, 91, 1, 645, 1253, 927],\n",
    "              [73, 8, 3215, 55, 927],\n",
    "              [711, 632, 71]]\n",
    "\n",
    "padded_inputs = pad_sequences(raw_inputs,padding='post')\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[24],\n",
    "     [24, 34],\n",
    "     [24, 34, 1],\n",
    "     [24, 34, 1, 9],\n",
    "     [24, 34, 1, 9, 56],\n",
    "     [24, 34, 1, 9, 56, 76],\n",
    "     [24, 34, 1, 9, 56, 76, 90],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67, 54],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]]\n",
    "\n",
    "import numpy as np\n",
    "np.max([len(x) for x in a])\n",
    "\n",
    "row = 11\n",
    "column = len(a)\n",
    "\n",
    "tensor = np.zeros((row,column))\n",
    "\n",
    "for i in range(len(a)):\n",
    "    lst = a[i]\n",
    "    for j in range(len(lst)):\n",
    "        tensor[i][j] = lst[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\n",
      "[24, 34]\n",
      "[24, 34, 1]\n",
      "[24, 34, 1, 9]\n",
      "[24, 34, 1, 9, 56]\n",
      "[24, 34, 1, 9, 56, 76]\n",
      "[24, 34, 1, 9, 56, 76, 90]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54, 14]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram from Sequence List\n",
    "\n",
    "lst = [24,34,1,9,56,76,90,11,67,54,14]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(0,len(lst)):\n",
    "    print(lst[:i+1])\n",
    "    data.append(lst[:i+1])\n",
    "\n",
    "# Padding\n",
    "pad_sequences(data,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 20]\n",
      " [ 1 12]\n",
      " [ 8  1]\n",
      " [ 9 14]\n",
      " [ 8  1]\n",
      " [ 9  3]]\n"
     ]
    }
   ],
   "source": [
    "# Truncating Sequence (Pre)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data_vec = [['3', '18', '9', '3', '11', '5', '20'],\n",
    "            ['3', '8', '1', '12'],\n",
    "            ['18', '1', '8', '1'],\n",
    "            ['8', '1', '9', '14'],\n",
    "            ['25', '1', '8', '1'],\n",
    "            ['9','3']]\n",
    "\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(data_vec, maxlen=2)\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 18]\n",
      " [ 3  8]\n",
      " [18  1]\n",
      " [ 8  1]\n",
      " [25  1]\n",
      " [ 9  3]]\n"
     ]
    }
   ],
   "source": [
    "# Truncating Sequence (Post)\n",
    "\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(data_vec, maxlen=2,truncating='post')\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union {'are', 'You', 'Hello', 'about', 'Hi', 'am', 'fine', 'How', 'I'}\n"
     ]
    }
   ],
   "source": [
    "# Union\n",
    "\n",
    "document1 = ['Hi', 'How','are','You', 'I','am']\n",
    "document2 = ['Hello','I','am','fine','How','about','You']\n",
    "\n",
    "#Union : words from two documents\n",
    "print(\"Union\",set(document1) | set(document2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection: {'You', 'am', 'How', 'I'}\n"
     ]
    }
   ],
   "source": [
    "# Intersection\n",
    "\n",
    "document1 = ['Hi', 'How','are','You', 'I','am']\n",
    "document2 = ['Hello','I','am','fine','How','about','You']\n",
    "\n",
    "#Intersection : common words across two documents\n",
    "print(\"Intersection:\",set(document1) & set(document2))       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence to Matrix (mode = binary and num_words = 10)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequences to Matrix (mode = count and num_words = 10)\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='count')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5         6         7         8    9\n",
       "0  0.0  0.4  0.2  0.2  0.2  0.0  0.000000  0.000000  0.000000  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.5  0.5  0.000000  0.000000  0.000000  0.0\n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.333333  0.333333  0.333333  0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequences to Matrix (mode = freq and num_words = 10)\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='freq')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_letters = abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;''\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \".,;''\"\n",
    "\n",
    "print(\"all_letters = {0}\".format(all_letters))\n",
    "\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrate Input for the RNN\n",
    "import numpy as np\n",
    "\n",
    "def name_intensor(name):\n",
    "    name_in_tensor = np.zeros((len(name),1,n_letters))\n",
    "    \n",
    "    for i,letter in enumerate(name):\n",
    "        name_in_tensor[i][0][all_letters.find(letter)] = 1\n",
    "    \n",
    "    return name_in_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(1, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "print(name_intensor('a'))\n",
    "print(name_intensor('a').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(2, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "print(name_intensor('af'))\n",
    "print(name_intensor('af').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(5, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "print(name_intensor('anand'))\n",
    "print(name_intensor('anand').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['klein', 'ist', 'das', 'Haus']\n",
      "['the', 'house', 'is', 'small']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Alignment([])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.api import AlignedSent\n",
    "\n",
    "algnsent = AlignedSent(['klein', 'ist', 'das', 'Haus'], ['the', 'house', 'is', 'small'])\n",
    "\n",
    "print(algnsent.words)\n",
    "\n",
    "print(algnsent.mots)\n",
    "\n",
    "algnsent.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8890388023094323\n",
      "0.061668228784061314\n",
      "0.11318685275683199\n",
      "0.07278526278031176\n",
      "['das', 'buch', 'ist', 'ja', 'klein']\n",
      "['the', 'book', 'is', 'small']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Alignment([(0, 0), (1, 1), (2, 2), (3, 2), (4, 3)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lexical translation model that ignores word orde\n",
    "# It is Used for Machine Translation\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.translate.ibm1 import IBMModel1\n",
    "from nltk.translate.api import AlignedSent\n",
    "\n",
    "bitext = []\n",
    "\n",
    "bitext.append(AlignedSent(['klein', 'ist', 'das', 'haus'], ['the', 'house', 'is', 'small']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'haus', 'ist', 'ja', 'groß'], ['the', 'house', 'is', 'big']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'buch', 'ist', 'ja', 'klein'], ['the', 'book', 'is', 'small']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'haus'], ['the', 'house']))\n",
    "\n",
    "bitext.append(AlignedSent(['das', 'buch'], ['the', 'book']))\n",
    "\n",
    "bitext.append(AlignedSent(['ein', 'buch'], ['a', 'book']))\n",
    "\n",
    "ibm1 = IBMModel1(bitext, 5)\n",
    "\n",
    "print(ibm1.translation_table['buch']['book'])\n",
    "\n",
    "print(ibm1.translation_table['das']['book'])\n",
    "\n",
    "print(ibm1.translation_table['buch'][None])\n",
    "\n",
    "print(ibm1.translation_table['ja'][None])\n",
    "\n",
    "test_sentence = bitext[2]\n",
    "\n",
    "print(test_sentence.words)\n",
    "\n",
    "print(test_sentence.mots)\n",
    "\n",
    "test_sentence.alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CBOW\n",
    "\n",
    "from keras.preprocessing import text\n",
    "\n",
    "norm_bible = ['beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters']\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "wids = [[w for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 6, 7, 2], [2, 8, 9, 10, 11, 3, 4, 12], [13, 1, 14, 3, 4, 15]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id\n",
    "\n",
    "# build vocabulary of unique words\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 6, 7, 2]\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '5'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "Input: [1, 6]\n",
      "Output: 5\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 6]\n",
      "context_words1 : [[1, 6]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '1'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '5'\n",
      "Word at range index at 0 is '5'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "Input: [5, 6, 7]\n",
      "Output: 1\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [5, 6, 7]\n",
      "context_words1 : [[5, 6, 7]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '6'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '5'\n",
      "Word at range index at 0 is '5'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 4 is '2'\n",
      "Input: [5, 1, 7, 2]\n",
      "Output: 6\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [5, 1, 7, 2]\n",
      "context_words1 : [[5, 1, 7, 2]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '7'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 4 is '2'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [1, 6, 2]\n",
      "Output: 7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 6, 2]\n",
      "context_words1 : [[1, 6, 2]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '2'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [6, 7]\n",
      "Output: 2\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [6, 7]\n",
      "context_words1 : [[6, 7]]\n",
      "*****************************\n",
      "###############################################################################\n",
      "[2, 8, 9, 10, 11, 3, 4, 12]\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '2'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "Input: [8, 9]\n",
      "Output: 2\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [8, 9]\n",
      "context_words1 : [[8, 9]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '8'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 0 is '2'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "Input: [2, 9, 10]\n",
      "Output: 8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [2, 9, 10]\n",
      "context_words1 : [[2, 9, 10]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '9'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 0 is '2'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "Input: [2, 8, 10, 11]\n",
      "Output: 9\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [2, 8, 10, 11]\n",
      "context_words1 : [[2, 8, 10, 11]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '10'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "Input: [8, 9, 11, 3]\n",
      "Output: 10\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [8, 9, 11, 3]\n",
      "context_words1 : [[8, 9, 11, 3]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '11'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "Input: [9, 10, 3, 4]\n",
      "Output: 11\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [9, 10, 3, 4]\n",
      "context_words1 : [[9, 10, 3, 4]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 5\n",
      "Word at index 5 : '3'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 3\n",
      "end: 8\n",
      "Choosing context word based on condition:\n",
      "[3, 4, 5, 6, 7]\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '12'\n",
      "Word at range index at 7 is '12'\n",
      "Input: [10, 11, 4, 12]\n",
      "Output: 3\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [10, 11, 4, 12]\n",
      "context_words1 : [[10, 11, 4, 12]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 6\n",
      "Word at index 6 : '4'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 4\n",
      "end: 9\n",
      "Choosing context word based on condition:\n",
      "[4, 5, 6, 7, 8]\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '12'\n",
      "Word at range index at 7 is '12'\n",
      "range index: 8\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [11, 3, 12]\n",
      "Output: 4\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [11, 3, 12]\n",
      "context_words1 : [[11, 3, 12]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 7\n",
      "Word at index 7 : '12'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 5\n",
      "end: 10\n",
      "Choosing context word based on condition:\n",
      "[5, 6, 7, 8, 9]\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 8\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 9\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [3, 4]\n",
      "Output: 12\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [3, 4]\n",
      "context_words1 : [[3, 4]]\n",
      "*****************************\n",
      "###############################################################################\n",
      "[13, 1, 14, 3, 4, 15]\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '13'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "Input: [1, 14]\n",
      "Output: 13\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 14]\n",
      "context_words1 : [[1, 14]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '1'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '13'\n",
      "Word at range index at 0 is '13'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "Input: [13, 14, 3]\n",
      "Output: 1\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [13, 14, 3]\n",
      "context_words1 : [[13, 14, 3]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '14'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '13'\n",
      "Word at range index at 0 is '13'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "Input: [13, 1, 3, 4]\n",
      "Output: 14\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [13, 1, 3, 4]\n",
      "context_words1 : [[13, 1, 3, 4]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '3'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '15'\n",
      "Word at range index at 5 is '15'\n",
      "Input: [1, 14, 4, 15]\n",
      "Output: 3\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 14, 4, 15]\n",
      "context_words1 : [[1, 14, 4, 15]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '4'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '15'\n",
      "Word at range index at 5 is '15'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [14, 3, 15]\n",
      "Output: 4\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [14, 3, 15]\n",
      "context_words1 : [[14, 3, 15]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 5\n",
      "Word at index 5 : '15'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 3\n",
      "end: 8\n",
      "Choosing context word based on condition:\n",
      "[3, 4, 5, 6, 7]\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [3, 4]\n",
      "Output: 15\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [3, 4]\n",
      "context_words1 : [[3, 4]]\n",
      "*****************************\n",
      "###############################################################################\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "\n",
    "for words in wids:\n",
    "    print(words)\n",
    "    sentence_length = len(words)\n",
    "    for index,word in enumerate(words):\n",
    "        print(\"sentence_length:\",sentence_length)\n",
    "        print(\"WindowSize:\",window_size)\n",
    "        print(\"index:\",index)\n",
    "        print(\"Word at index {0} : '{1}'\".format(index,word))\n",
    "        \n",
    "        context_words =[]\n",
    "        context_words1 = []\n",
    "       \n",
    "        print(\"start = index - window_size\")\n",
    "        print(\"end = index + window_size + 1\")\n",
    "        \n",
    "        start = index - window_size\n",
    "        end = index + window_size + 1\n",
    "        \n",
    "\n",
    "        print(\"start:\",start)\n",
    "        print(\"end:\",end)\n",
    "       \n",
    "        print(\"Choosing context word based on condition:\")\n",
    "        \n",
    "        print([x for x in range(start, end)])\n",
    "        for i in range(start, end):\n",
    "            \n",
    "            print(\"range index:\",i)\n",
    "            \n",
    "            if 0 <= i:\n",
    "                print(\"Statisfied the condition '0 <= i' \")\n",
    "                \n",
    "                if i < sentence_length:\n",
    "                    print(\"Statisfied the condition 'i < sentence_length' \")\n",
    "                    \n",
    "                    if i != index:\n",
    "                        print(\"Statisfied the condition 'i != index'\")\n",
    "                        print(\"Selected Word: '{0}'\".format(words[i]))\n",
    "            \n",
    "            if  0 <= i < sentence_length and i != index:\n",
    "                print(\"Word at range index at {0} is '{1}'\".format(i,words[i]))\n",
    "                context_words.append(words[i])\n",
    "                \n",
    "        \n",
    "        print(\"Input:\",context_words)   \n",
    "        print(\"Output:\",words[index])\n",
    "        \n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    \n",
    "        context_words1.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"context_words :\",context_words)\n",
    "        print(\"context_words1 :\",context_words1)\n",
    "        print(\"*****************************\")\n",
    "    print(\"###############################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "def generator():\n",
    "    for words in wids:\n",
    "        sentence_length = len(words)\n",
    "        for index,word in enumerate(words):\n",
    "            context_words =[]\n",
    "            label_word = []\n",
    "\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
    "\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context_words, maxlen=7)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[0 0 0 0 0 1 6]]  output: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 5 6 7]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 5 1 7 2]]  output: [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 1 6 2]]  output: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 6 7]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 8 9]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0  2  9 10]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  2  8 10 11]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  8  9 11  3]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  9 10  3  4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0 10 11  4 12]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0 11  3 12]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0  0  1 14]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "input: [[ 0  0  0  0 13 14  3]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0 13  1  3  4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "input: [[ 0  0  0  1 14  4 15]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0 14  3 15]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for x ,y in generator():\n",
    "    print(\"input: {0}  output: {1}\".format(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(heaven (7), beginning (5)) -> 1\n",
      "(heaven (7), created (6)) -> 1\n",
      "(god (1), beginning (5)) -> 1\n",
      "(heaven (7), earth (2)) -> 1\n",
      "(beginning (5), heaven (7)) -> 1\n",
      "(beginning (5), god (1)) -> 0\n",
      "(beginning (5), void (10)) -> 0\n",
      "(beginning (5), god (1)) -> 1\n",
      "(beginning (5), earth (2)) -> 1\n",
      "(god (1), created (6)) -> 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 16\n",
      "Vocabulary Sample: [('god', 1), ('earth', 2), ('upon', 3), ('face', 4), ('beginning', 5), ('created', 6), ('heaven', 7), ('without', 8), ('form', 9), ('void', 10)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1 \n",
    "embed_size = 100\n",
    "\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Using Glove : Pretained Word Embedding\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "corpus = [\n",
    "            'This is an excellent movie',\n",
    "            'The move was fantastic I like it',\n",
    "            'You should watch it is brilliant',\n",
    "            'Exceptionally good',\n",
    "            'Wonderfully directed and executed I like it',\n",
    "            'Its a fantastic series',\n",
    "            'Never watched such a brillent movie',\n",
    "            'It is a Wonderful movie',\n",
    "        ]\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 2, 9, 10, 3],\n",
       " [11, 12, 13, 5, 6, 7, 1],\n",
       " [14, 15, 16, 1, 2, 17],\n",
       " [18, 19],\n",
       " [20, 21, 22, 23, 6, 7, 1],\n",
       " [24, 4, 5, 25],\n",
       " [26, 27, 28, 4, 29, 3],\n",
       " [1, 2, 4, 30, 3]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_sentences = word_tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  2  9 10  3  0  0]\n",
      " [11 12 13  5  6  7  1]\n",
      " [14 15 16  1  2 17  0]\n",
      " [18 19  0  0  0  0  0]\n",
      " [20 21 22 23  6  7  1]\n",
      " [24  4  5 25  0  0  0]\n",
      " [26 27 28  4 29  3  0]\n",
      " [ 1  2  4 30  3  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_count = lambda sentence: len(word_tokenize(sentence))\n",
    "longest_sentence = max(corpus, key=word_count)\n",
    "length_long_sentence = len(word_tokenize(longest_sentence))\n",
    "\n",
    "padded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\n",
    "\n",
    "print(padded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Pretrained Word Embeddings : Glove\n",
    "\n",
    "# The smallest file is named \"Glove.6B.zip\". The size of the file is 822 MB. \n",
    "# The file contains 50, 100, 200, and 300 dimensional word vectors for 400k words. We will be using the 100 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionary that will store our word embeddings\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "# Loading the Pretrained Glove Word Embedding Model\n",
    "glove_file = open('C:/MyWork/MyLearning/Career Growth/ML/Files/DataSet/glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Here notice that we loaded glove.6B.100d.txt file. This file contains 100 dimensional word embeddings. \n",
    "\n",
    "# We will create a dictionary that will contain words as keys and the corresponding 100 dimensional vectors as values, \n",
    "#  in the form of an array.\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "    \n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dictionary embeddings_dictionary now contains words and corresponding GloVe embeddings for all the words\n",
    "\n",
    "embeddings_dictionary['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.1766   ,  0.093851 ,  0.24351  ,  0.44313  , -0.39037  ,\n",
       "        0.12524  , -0.19918  ,  0.59855  , -0.82035  ,  0.28006  ,\n",
       "        0.54231  ,  0.023079 ,  0.12837  , -0.044489 ,  0.3837   ,\n",
       "       -0.75659  ,  0.40254  , -0.4462   , -0.81599  , -0.0091513,\n",
       "        0.65219  , -0.043656 ,  0.54919  , -0.16696  ,  0.73028  ,\n",
       "       -0.20703  , -0.069863 , -0.31259  ,  0.27226  ,  0.084905 ,\n",
       "       -0.60498  ,  0.42826  ,  0.60134  ,  0.50953  , -0.39073  ,\n",
       "        0.44608  , -0.36331  ,  0.50858  , -0.20308  , -0.43503  ,\n",
       "       -0.086827 , -0.86581  , -1.0151   , -0.35725  , -0.12993  ,\n",
       "        0.3324   ,  0.3026   ,  0.067277 , -0.52948  , -0.81223  ,\n",
       "        0.39562  , -0.79537  ,  0.24331  ,  1.2506   , -1.0169   ,\n",
       "       -3.3391   , -0.79691  , -0.33877  ,  1.366    ,  0.87513  ,\n",
       "       -0.63701  ,  0.68381  , -0.057432 ,  0.12541  , -0.8258   ,\n",
       "       -0.56117  ,  0.30807  ,  0.1545   ,  0.61473  ,  0.67403  ,\n",
       "       -0.60833  , -0.25911  , -0.35619  , -0.71189  , -0.31207  ,\n",
       "        0.035238 ,  0.22488  , -0.33492  , -1.1586   , -0.17373  ,\n",
       "        0.95937  ,  0.24479  , -0.46205  , -0.075941 , -1.0844   ,\n",
       "        0.093676 ,  0.48546  ,  0.13008  ,  0.23455  , -0.27964  ,\n",
       "       -0.24481  , -0.016213 ,  0.46302  ,  1.0291   , -0.81817  ,\n",
       "        0.17522  ,  0.06797  ,  0.056305 ,  1.2312   ,  0.40695  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dictionary['at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:it and index:1\n",
      "[-3.0664e-01  1.6821e-01  9.8511e-01 -3.3606e-01 -2.4160e-01  1.6186e-01\n",
      " -5.3496e-02  4.3010e-01  5.7342e-01 -7.1569e-02  3.6101e-01  2.6729e-01\n",
      "  2.7789e-01 -7.2268e-02  1.3838e-01 -2.6714e-01  1.2999e-01  2.2949e-01\n",
      " -1.8311e-01  5.0163e-01  4.4921e-01 -2.0821e-02  4.2642e-01 -6.8762e-02\n",
      "  4.0337e-01  9.5198e-02 -3.1944e-01 -5.4651e-01 -1.3345e-01 -5.6511e-01\n",
      " -2.0975e-01  1.1592e+00 -1.9400e-01  1.9828e-01 -1.1924e-01  4.1781e-01\n",
      "  6.8383e-03 -2.0537e-01 -5.3375e-01 -5.2225e-01 -3.8227e-01 -6.5833e-03\n",
      "  1.4265e-01 -4.2502e-01 -3.1150e-01  2.7352e-03  7.5093e-01 -4.8218e-01\n",
      " -1.8595e-01 -7.7104e-01 -4.6406e-02 -6.9140e-02  4.1688e-01  1.3235e+00\n",
      " -8.1742e-01 -3.3998e+00 -1.1307e-01 -3.4123e-01  2.0775e+00  6.1369e-01\n",
      "  1.4792e-01  9.3753e-01 -1.0138e-01  2.8426e-01  9.7899e-01 -3.2335e-01\n",
      "  6.3697e-01  5.8308e-01  2.2820e-01 -3.1696e-01  2.1061e-01 -6.5060e-01\n",
      "  2.1653e-01 -2.4347e-01  5.5519e-01 -3.4351e-01 -9.5093e-02 -1.4715e-01\n",
      " -1.2876e+00  3.9310e-01  3.0163e-01 -2.1767e-01 -1.1146e+00  5.1349e-01\n",
      " -1.3410e+00 -3.0381e-01  3.2499e-01 -4.5236e-01 -1.7743e-01 -4.8504e-02\n",
      " -1.2178e-01 -4.2108e-01 -4.0327e-01  3.8452e-02 -3.6084e-01  3.7738e-02\n",
      " -2.1885e-01 -3.8775e-01  3.6916e-01  5.4521e-01]\n",
      "word:is and index:2\n",
      "[-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      " -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "  0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "  0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "  0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      " -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "  0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "  0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      " -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      " -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "  0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "  0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      " -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      " -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      " -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      " -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      " -0.044219  -1.2969     0.76217    0.46349  ]\n",
      "word:movie and index:3\n",
      "[ 0.38251    0.14821    0.60601   -0.51533    0.43992    0.061053\n",
      " -0.62716   -0.025385   0.1643    -0.22101    0.14423   -0.37213\n",
      " -0.21683   -0.08895    0.097904   0.6561     0.64455    0.47698\n",
      "  0.83849    1.6486     0.88922   -0.1181    -0.012465  -0.52082\n",
      "  0.77854    0.48723   -0.014991  -0.14127   -0.34747   -0.29595\n",
      "  0.1028     0.57191   -0.045594   0.026443   0.53816    0.32257\n",
      "  0.40788   -0.043599  -0.146     -0.48346    0.32036    0.55086\n",
      " -0.76259    0.43269    0.61753   -0.36503   -0.60599   -0.79615\n",
      "  0.3929    -0.23668   -0.34719   -0.61201    0.54747    0.94812\n",
      "  0.20941   -2.7771    -0.6022     0.8495     1.2549     0.017893\n",
      " -0.041901   2.1147    -0.026618  -0.28104    0.68124   -0.14165\n",
      "  0.99249    0.49879   -0.67538    0.6417     0.42303   -0.27913\n",
      "  0.063403   0.68909   -0.36183    0.053709  -0.16806    0.19422\n",
      " -0.47073   -0.14803   -0.58986   -0.2797     0.16792    0.10568\n",
      " -1.7601     0.0088254 -0.83326   -0.5836    -0.37079   -0.56591\n",
      "  0.20699    0.071315   0.055586  -0.29757   -0.072659  -0.25596\n",
      "  0.42688    0.058921   0.091112   0.47283  ]\n",
      "word:a and index:4\n",
      "[-0.27086    0.044006  -0.02026   -0.17395    0.6444     0.71213\n",
      "  0.3551     0.47138   -0.29637    0.54427   -0.72294   -0.0047612\n",
      "  0.040611   0.043236   0.29729    0.10725    0.40156   -0.53662\n",
      "  0.033382   0.067396   0.64556   -0.085523   0.14103    0.094539\n",
      "  0.74947   -0.194     -0.68739   -0.41741   -0.22807    0.12\n",
      " -0.48999    0.80945    0.045138  -0.11898    0.20161    0.39276\n",
      " -0.20121    0.31354    0.75304    0.25907   -0.11566   -0.029319\n",
      "  0.93499   -0.36067    0.5242     0.23706    0.52715    0.22869\n",
      " -0.51958   -0.79349   -0.20368   -0.50187    0.18748    0.94282\n",
      " -0.44834   -3.6792     0.044183  -0.26751    2.1997     0.241\n",
      " -0.033425   0.69553   -0.64472   -0.0072277  0.89575    0.20015\n",
      "  0.46493    0.61933   -0.1066     0.08691   -0.4623     0.18262\n",
      " -0.15849    0.020791   0.19373    0.063426  -0.31673   -0.48177\n",
      " -1.3848     0.13669    0.96859    0.049965  -0.2738    -0.035686\n",
      " -1.0577    -0.24467    0.90366   -0.12442    0.080776  -0.83401\n",
      "  0.57201    0.088945  -0.42532   -0.018253  -0.079995  -0.28581\n",
      " -0.01089   -0.4923     0.63687    0.23642  ]\n",
      "word:fantastic and index:5\n",
      "[ 1.1512e-01  4.1563e-01  8.7816e-02 -5.3405e-01 -2.8153e-01 -1.1687e-02\n",
      "  5.2698e-02  3.9530e-01 -4.6913e-01 -3.1560e-01  6.6562e-01 -3.9821e-01\n",
      " -2.3686e-01 -9.0550e-01 -8.2461e-02  3.9941e-01 -2.3861e-01  8.6388e-01\n",
      "  4.3190e-02  8.6083e-01  6.2514e-01  1.0869e-01 -4.6694e-01 -7.8636e-01\n",
      "  4.0516e-01  3.2081e-01  1.4117e-01  3.4725e-01  5.3938e-02 -5.6039e-01\n",
      " -8.0535e-01  2.0277e-01 -6.6209e-01 -5.4528e-01  1.8393e-01  1.3221e-01\n",
      " -8.3765e-01  5.2724e-01 -1.2740e-01 -6.4205e-01  6.8566e-02  5.9769e-01\n",
      "  2.8973e-01 -8.8062e-02  1.8834e-01  5.2193e-01  3.5441e-01  1.0297e-01\n",
      "  2.4525e-02 -2.9434e-01 -1.6666e-01 -6.6385e-02  3.5745e-01  6.5942e-01\n",
      "  7.1508e-02 -1.8177e+00  2.9155e-01  7.9974e-01 -1.0634e-01 -1.3563e-01\n",
      " -3.5376e-01  9.5492e-01 -1.4421e+00  3.6402e-03  4.7339e-01 -2.2880e-01\n",
      "  1.7273e-02  1.5761e-01 -1.9878e-02 -6.6218e-01  4.7727e-01  5.3783e-02\n",
      "  4.2225e-01  1.8487e-04  4.2256e-01  5.4570e-01  3.2446e-01 -2.8863e-01\n",
      "  6.1480e-02  5.7642e-01  4.0662e-02  4.5600e-01  2.5457e-01 -8.6675e-02\n",
      " -7.4609e-01 -7.9204e-02 -1.7898e-01 -1.7868e-01 -5.3118e-01 -5.2360e-01\n",
      " -1.1393e-01 -1.0532e-01  2.4266e-01  6.6722e-02 -1.4769e-01 -3.2199e-01\n",
      " -6.1513e-01 -4.3121e-01  2.5169e-02  5.5207e-01]\n",
      "word:i and index:6\n",
      "[-0.046539   0.61966    0.56647   -0.46584   -1.189      0.44599\n",
      "  0.066035   0.3191     0.14679   -0.22119    0.79239    0.29905\n",
      "  0.16073    0.025324   0.18678   -0.31001   -0.28108    0.60515\n",
      " -1.0654     0.52476    0.064152   1.0358    -0.40779   -0.38011\n",
      "  0.30801    0.59964   -0.26991   -0.76035    0.94222   -0.46919\n",
      " -0.18278    0.90652    0.79671    0.24825    0.25713    0.6232\n",
      " -0.44768    0.65357    0.76902   -0.51229   -0.44333   -0.21867\n",
      "  0.3837    -1.1483    -0.94398   -0.15062    0.30012   -0.57806\n",
      "  0.20175   -1.6591    -0.079195   0.026423   0.22051    0.99714\n",
      " -0.57539   -2.7266     0.31448    0.70522    1.4381     0.99126\n",
      "  0.13976    1.3474    -1.1753     0.0039503  1.0298     0.064637\n",
      "  0.90887    0.82872   -0.47003   -0.10575    0.5916    -0.4221\n",
      "  0.57331   -0.54114    0.10768    0.39784   -0.048744   0.064596\n",
      " -0.61437   -0.286      0.5067    -0.49758   -0.8157     0.16408\n",
      " -1.963     -0.26693   -0.37593   -0.95847   -0.8584    -0.71577\n",
      " -0.32343   -0.43121    0.41392    0.28374   -0.70931    0.15003\n",
      " -0.2154    -0.37616   -0.032502   0.8062   ]\n",
      "word:like and index:7\n",
      "[-0.2687    0.81708   0.69896  -0.72341   0.091566  0.19557  -0.52112\n",
      " -0.24313  -0.44701  -0.27039  -0.34126  -0.46898   0.42583   0.46289\n",
      "  0.17106  -0.26795   0.23162   0.46568  -0.31808   0.75875   0.31857\n",
      "  0.64124   0.067042 -0.18517   0.49996   0.36964  -0.31172  -0.73098\n",
      " -0.26902  -0.32058   0.23394   0.24276   0.1426   -0.2793    0.38823\n",
      "  0.42398   0.1021    0.33316   0.3015   -0.52711  -0.024475 -0.15301\n",
      " -0.3224   -0.51231  -0.5525    0.29819   0.10847   0.052334 -0.2298\n",
      " -0.77889  -0.08928   0.48109   0.015368  0.92544  -0.26122  -2.4759\n",
      " -0.019825  0.58281   1.306     0.73512  -0.34372   1.5829   -0.10814\n",
      "  0.11388   0.7922    0.18347   1.2232    0.35697   0.17504  -0.16527\n",
      " -0.012827 -0.47918  -0.32111  -0.40573  -0.37151   0.086323  0.25172\n",
      " -0.082751 -0.25584  -0.19178   1.0474   -0.51984  -0.71463   0.38827\n",
      " -1.6722    0.015986 -0.22668  -0.26602  -0.57925  -0.85651   0.20543\n",
      " -0.46372  -0.065652 -0.061944 -0.57233  -0.46406  -0.41405  -0.4011\n",
      "  0.74657   0.31122 ]\n",
      "word:this and index:8\n",
      "[-0.57058   0.44183   0.70102  -0.41713  -0.34058   0.02339  -0.071537\n",
      "  0.48177  -0.013121  0.16834  -0.13389   0.040626  0.15827  -0.44342\n",
      " -0.019403 -0.009661 -0.046284  0.093228 -0.27331   0.2285    0.33089\n",
      " -0.36474   0.078741  0.3585    0.44757  -0.2299    0.18077  -0.6265\n",
      "  0.053852 -0.29154  -0.4256    0.62903   0.14393  -0.046004 -0.21007\n",
      "  0.48879  -0.057698  0.37431  -0.030075 -0.34494  -0.29702   0.15095\n",
      "  0.28248  -0.16578   0.076131 -0.093016  0.79365  -0.60489  -0.18874\n",
      " -1.0173    0.31962  -0.16344   0.54177   1.1725   -0.47875  -3.3842\n",
      " -0.081301 -0.3528    1.8372    0.44516  -0.52666   0.99786  -0.32178\n",
      "  0.033462  1.1783   -0.072905  0.39737   0.26166   0.33111  -0.35629\n",
      " -0.16558  -0.44382  -0.14183  -0.37976   0.28994  -0.029114 -0.35169\n",
      " -0.27694  -1.344     0.19555   0.16887   0.040237 -0.80212   0.23366\n",
      " -1.3837   -0.023132  0.085395 -0.74051  -0.073934 -0.58838  -0.085735\n",
      " -0.10525  -0.51571   0.15038  -0.16694  -0.16372  -0.22702  -0.66102\n",
      "  0.47197   0.37253 ]\n",
      "word:an and index:9\n",
      "[-0.4214   -0.18797   0.46241  -0.17605   0.36212   0.36701   0.27924\n",
      "  0.14634  -0.054227  0.45834   0.065416 -0.33725   0.067505 -0.36316\n",
      "  0.50302  -0.010361  0.72826  -0.17564  -0.33996   0.072864  0.64481\n",
      " -0.23908   0.38383   0.13858   1.0994   -0.24883  -0.15078  -0.48738\n",
      " -0.23042   0.064788 -0.70183   0.82654   0.06128   0.18531  -0.30162\n",
      " -0.022151  0.34302   0.80331   0.17135   0.15462  -0.50759   0.39572\n",
      "  0.054291 -0.53081   0.48252   0.086205  0.59585  -0.22377  -0.3955\n",
      " -0.73036  -0.10279  -0.39166   1.229     1.2129   -1.0365   -3.4971\n",
      "  0.10923  -1.0084    1.9998    0.7964    0.3881    0.43746   0.085194\n",
      "  0.38549   0.61993  -1.032     0.70119  -0.2246    0.079435  0.09126\n",
      " -0.21196  -0.55429  -0.053352 -0.80201   0.46798  -0.05005  -0.57422\n",
      " -0.084822 -1.7227   -0.94286   0.98667   0.31211  -0.37735   0.068674\n",
      " -0.77838  -0.28486   0.81047   0.46596  -0.11865  -0.93411   0.33722\n",
      "  0.037906 -0.18273  -0.019941  0.20494  -0.47718  -0.49253  -0.56518\n",
      "  0.72558  -0.15913 ]\n",
      "word:excellent and index:10\n",
      "[-0.2816     0.18427   -0.06755    0.27694   -0.066775  -0.41389\n",
      "  0.30757   -0.11097   -0.84585   -0.17047    0.0062422 -0.65395\n",
      "  0.28771   -0.12409   -0.26717    0.026893   0.17115   -0.46256\n",
      "  0.19549    1.1399    -0.46206    0.39222   -0.18622   -0.53259\n",
      "  0.073297   0.0045262 -0.45476    0.16952   -0.41111   -0.31766\n",
      " -0.73616    0.56228   -0.26528   -0.088054   0.93175    0.46633\n",
      " -0.36245    0.46954   -0.18022   -0.07036    0.55793    0.13965\n",
      "  0.38983   -0.04636    0.55198    0.020288   0.34741   -0.61839\n",
      "  0.051404  -0.83699    0.05628   -0.24738   -0.19872    0.75093\n",
      "  0.068948  -1.7575     0.70621    0.079792   1.1462    -0.63423\n",
      " -0.68378    0.51682   -0.68141    0.29451    0.25864   -0.25839\n",
      "  0.36467    0.026786   0.35151   -0.13273    0.18513    0.52367\n",
      "  1.0416    -0.12293    0.77957    0.41305   -0.28948    0.19722\n",
      "  0.41088   -0.35153    0.48837    0.58854   -0.37273   -0.45631\n",
      " -0.98976   -0.070993   0.48845   -0.082542  -0.49009   -0.29484\n",
      "  0.031072  -0.38779    0.15321   -0.0078053 -0.43276    0.37334\n",
      " -0.76888   -0.82518    0.28753    0.76069  ]\n",
      "word:the and index:11\n",
      "[-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "word:move and index:12\n",
      "[-0.021606  -0.10639    0.11104   -0.19535   -0.15463   -0.021794\n",
      " -0.49271    0.037687   0.03788    0.079461   0.11757    0.21303\n",
      "  0.057177  -0.59581   -0.12174    0.058865  -0.3029     0.33552\n",
      " -0.41281   -0.069985   0.75037    0.11583   -0.27081    0.46937\n",
      "  0.21231   -0.11562   -0.21439   -0.33636    0.12305    0.019461\n",
      " -0.39404    0.8718    -0.17674   -0.13337    0.0030434  0.27575\n",
      "  0.22314   -0.37808   -0.35052   -0.23319   -0.4751    -0.54627\n",
      " -0.21481   -0.35311   -0.25409    0.13777    0.088258   0.27232\n",
      "  0.17943   -0.79158    0.4327     0.029326  -0.0076881  1.2184\n",
      "  0.048029  -2.5612    -0.24577   -0.39375    1.6298     0.23657\n",
      " -0.46253    0.49255   -0.16006   -0.01842    0.88971   -0.022171\n",
      " -0.19875    0.72445   -0.22658   -0.56365    0.083209  -0.90564\n",
      " -0.39346   -1.0547     0.35143   -0.32817   -0.2591     0.39502\n",
      " -0.2802     0.15717    0.42952   -0.26731   -0.56228   -0.35319\n",
      " -1.026     -0.51456    0.70897    0.23269    0.21666   -0.69723\n",
      " -0.43846   -0.17088   -0.39565    0.060127  -0.72325   -0.16621\n",
      "  0.28695    0.11868    0.6948     0.21722  ]\n",
      "word:was and index:13\n",
      "[ 1.3717e-01 -5.4287e-01  1.9419e-01 -2.9953e-01  1.7545e-01  8.4672e-02\n",
      "  6.7752e-01  9.8295e-02 -3.5611e-02  2.1334e-01  5.1663e-01  2.0687e-01\n",
      "  4.4082e-01 -3.3655e-01  5.6025e-01 -6.8790e-01  5.1957e-01 -2.1258e-01\n",
      " -5.2708e-01 -1.2249e-01  3.3099e-01  2.6448e-02  5.9007e-01  6.5469e-03\n",
      "  4.5405e-01 -3.3884e-01 -2.8261e-01 -2.4633e-01  1.0847e-01  3.1640e-01\n",
      " -1.5368e-01  7.3503e-01  1.1858e-01  7.0842e-01  7.5081e-02  2.9738e-01\n",
      " -1.1395e-01  4.0807e-01 -4.2531e-02 -2.1301e-01 -7.9849e-01 -1.2703e-01\n",
      "  7.5200e-01 -4.1746e-01  4.6615e-01 -3.9097e-02  6.5961e-01 -3.2336e-01\n",
      "  4.4200e-01 -9.4137e-01 -2.3125e-01 -3.0604e-01  7.9912e-01  1.4581e+00\n",
      " -8.8199e-01 -3.0041e+00 -7.5243e-01 -2.0503e-01  1.1998e+00  9.4881e-01\n",
      "  3.0649e-01  4.8411e-01 -7.5720e-01  6.5856e-01  7.0107e-01 -9.3141e-01\n",
      "  5.2928e-01  2.3323e-01  1.8857e-01  3.8691e-01  1.1489e-02 -3.1937e-01\n",
      "  1.1858e-02  2.2944e-01  1.7764e-01  1.6868e-01  1.4003e-01  5.8647e-01\n",
      " -1.5447e+00 -6.4425e-02 -6.4711e-04  1.3606e-01 -3.2695e-01  1.0043e-01\n",
      " -1.5460e+00 -5.4760e-01  2.1027e-01 -6.7195e-01 -1.5970e-01 -6.8271e-01\n",
      " -2.2043e-01 -8.7088e-01 -1.6248e-01  8.3086e-01 -2.3045e-01  1.9864e-01\n",
      " -5.1892e-02 -5.2057e-01  2.5434e-01 -2.3759e-01]\n",
      "word:you and index:14\n",
      "[-0.49886    0.76602    0.89751   -0.78547   -0.6855     0.62609\n",
      " -0.39655    0.34913    0.33334   -0.45233    0.61223    0.075948\n",
      "  0.22531    0.16365    0.28095   -0.24758    0.0099009  0.71108\n",
      " -0.75859    0.87423    0.0031041  0.35796   -0.35233   -0.665\n",
      "  0.38447    0.62677   -0.51543   -0.96653    0.61517   -0.75455\n",
      " -0.012359   1.1188     0.35719    0.0071769  0.20255    0.5011\n",
      " -0.44046    0.10661    0.79391   -0.80948   -0.015601  -0.22888\n",
      " -0.34198   -1.0065    -0.8763     0.15165   -0.085339  -0.6465\n",
      " -0.16733   -1.4499    -0.0065905  0.0048113 -0.012445   1.0474\n",
      " -0.19381   -2.5991     0.40528    0.43803    1.9332     0.45814\n",
      " -0.048819   1.4308    -0.78639   -0.20792    1.09       0.24816\n",
      "  1.1487     0.51481   -0.21832   -0.4572     0.13888   -0.26369\n",
      "  0.13647   -0.60539    0.099586   0.23344    0.13647   -0.1846\n",
      " -0.047734  -0.18392    0.52719   -0.2885    -1.0742    -0.0467\n",
      " -1.8302    -0.21197    0.0298    -0.30964   -0.43386   -0.36463\n",
      " -0.32738   -0.0093427  0.47205   -0.51691   -0.59176   -0.32343\n",
      "  0.20052   -0.41179    0.40539    0.78504  ]\n",
      "word:should and index:15\n",
      "[-0.044737   0.47515    0.26053   -0.39256   -0.29304    0.0071112\n",
      " -0.44643    0.53438    0.19748   -0.23917   -0.27637    0.32076\n",
      "  0.7137     0.089464  -0.29348   -0.62618    0.20319    0.65568\n",
      " -1.2966    -0.02145    0.075899  -0.17694    0.093996  -0.089708\n",
      " -0.35753    0.12062   -0.17008   -0.61405    0.027099  -0.29515\n",
      "  0.10724    0.82951   -0.15541    0.31132    0.50066    0.5156\n",
      "  0.2626     0.18109   -0.33884   -0.38865   -0.7261    -0.41048\n",
      "  0.12704   -0.67029   -0.30651   -0.27046    0.25184   -0.38691\n",
      " -0.2838    -1.4529     0.42155   -0.20488   -0.26328    1.3288\n",
      " -0.12419   -1.978      0.16039   -0.071989   1.7647     0.56619\n",
      " -0.14267    0.56513   -0.72975   -0.13564    1.2719     0.38483\n",
      "  0.11503    0.82408   -0.26384   -0.73986    0.39614   -0.66244\n",
      "  0.061608  -0.36869    0.065238  -0.22564   -0.42271    0.26272\n",
      " -0.63686   -0.067737   0.5745    -0.36493   -0.61019   -0.13889\n",
      " -1.9403    -0.28439    0.37524    0.16418   -0.024866  -0.55427\n",
      " -0.58489   -0.49175   -0.27284   -0.39679   -0.40491    0.35964\n",
      "  0.37232   -0.74777    0.37442    0.32655  ]\n",
      "word:watch and index:16\n",
      "[-0.38264   -0.089682   0.024723  -0.75717   -0.47556    0.61837\n",
      " -0.48873    0.61044   -0.31818   -0.63448    0.29585   -0.016296\n",
      "  0.26651   -0.57651    0.037018   0.27653    0.3202     0.34668\n",
      " -0.14257    0.079186  -0.027993   0.1436     0.32445   -0.34822\n",
      "  0.57161    0.2056     0.0052677 -0.0092796  0.7061     0.19237\n",
      "  0.17404   -0.014847   0.096184   0.75065    0.38236    0.11718\n",
      " -0.4454    -0.018317   0.67536   -0.60288   -0.30405    0.023487\n",
      " -0.36481   -0.37409   -0.31277   -0.028718  -0.12319   -0.28882\n",
      " -0.18277   -1.2734     0.43808   -0.058079  -0.11516    0.73796\n",
      "  0.099975  -2.0191    -0.10777    0.20513    1.5224     0.16566\n",
      " -0.40803    1.171     -0.015485  -0.62267    0.29923    0.51416\n",
      " -0.042769   0.6814     0.12516    0.48709    0.27994   -0.45281\n",
      "  0.38514   -0.16599    0.029629   0.10676    0.15784    0.16597\n",
      "  0.1163    -0.1154     0.71826   -0.083241  -0.15124    0.30054\n",
      " -1.1409    -0.59024   -0.50591   -0.026484   0.35409   -0.11742\n",
      "  0.62724   -0.29372    0.41494   -0.914     -0.3828     0.046055\n",
      "  0.024563   0.16366    0.43887    0.28925  ]\n",
      "word:brilliant and index:17\n",
      "[ 0.0099164 -0.10068   -0.16974   -0.12977    0.36971    0.59249\n",
      "  0.39798   -0.32726   -1.3741     0.16476    0.19944   -0.37969\n",
      "  0.066111  -0.38407    0.074161   0.2444    -0.13606    0.06561\n",
      "  0.49232    0.47116    0.16288    0.15274   -0.076117  -0.36021\n",
      "  0.86973    0.51174   -0.47599    0.38428    0.054939  -0.69243\n",
      " -0.79634    0.56333   -0.39427   -0.6077     0.073616  -0.31645\n",
      " -0.86014   -0.027097   0.14168    0.12978    0.01516    0.48919\n",
      "  0.1385     0.145      0.26751   -0.056297   0.6465     0.23188\n",
      "  0.049377  -0.29863   -0.010632  -0.29615    0.25721    0.59271\n",
      " -0.31223   -1.669      0.0020522  0.78035    0.46117   -0.49472\n",
      " -0.38064    0.93538   -0.72728    0.19933    0.36566   -0.68299\n",
      "  0.83521   -0.019304  -0.50038   -0.41469    0.31883    0.46438\n",
      "  0.025088  -0.12106    0.26883   -0.052977  -0.37666   -0.0075125\n",
      "  0.44219   -0.16636    0.79712    0.61516    0.0062811 -0.36103\n",
      " -1.0715    -0.32515    0.36246    0.1244    -0.22838   -0.47589\n",
      "  0.2362    -0.1163     0.041312   0.27259   -0.5754     0.29581\n",
      " -1.0477    -0.74512   -0.8182     0.55804  ]\n",
      "word:exceptionally and index:18\n",
      "[-7.7494e-01 -5.7527e-02 -2.3492e-01  3.3946e-03 -1.7531e-01 -2.6639e-01\n",
      " -4.4507e-01  5.8603e-01 -1.0198e+00 -9.8749e-02 -6.4086e-02 -2.1140e-01\n",
      "  7.1104e-04 -1.3723e-01 -2.9789e-01 -4.2275e-01 -6.2442e-01 -5.1619e-01\n",
      "  4.1576e-01  5.7435e-01 -1.6880e-01  3.1748e-01  2.2373e-01 -2.1491e-01\n",
      "  1.4634e-01  4.3115e-02 -5.2553e-01 -6.2675e-02 -7.0615e-01  3.1047e-01\n",
      " -1.9507e-01 -2.8654e-01  1.2981e-01 -2.7591e-02  8.1388e-01  3.7132e-01\n",
      "  2.0763e-01 -2.9620e-01 -1.3592e-01  2.1216e-01  5.3947e-01  4.0986e-01\n",
      "  1.1549e-01 -2.8280e-01  6.4589e-01 -5.2403e-01  1.3634e+00  2.8790e-01\n",
      " -6.7226e-03 -8.9058e-01  1.1425e-01 -7.4913e-01  5.3187e-01  3.6296e-01\n",
      " -2.9046e-01 -8.9690e-01  8.2124e-01 -1.6931e-01  9.3165e-01 -2.2517e-01\n",
      " -9.7050e-02  8.0300e-01 -8.7695e-01  2.2499e-01  4.2062e-01 -6.0309e-01\n",
      "  2.4520e-01 -4.4861e-01  9.1279e-01 -3.7197e-01  4.0374e-01 -1.9384e-01\n",
      "  1.3265e-01  4.9278e-01  7.8423e-01  2.2307e-02 -5.9613e-01  6.8876e-02\n",
      "  2.3564e-01 -5.4018e-01  6.9930e-01  5.1146e-01 -6.7883e-01 -1.0573e-01\n",
      " -6.7241e-01 -6.6017e-01  5.0591e-01 -5.4890e-02 -5.4079e-01 -3.2005e-01\n",
      " -1.2563e-01  2.2572e-01 -1.2549e-01  3.6818e-02  2.8085e-01 -3.0808e-02\n",
      " -8.4468e-01 -5.0869e-01 -7.5295e-01 -3.1467e-01]\n",
      "word:good and index:19\n",
      "[-0.030769   0.11993    0.53909   -0.43696   -0.73937   -0.15345\n",
      "  0.081126  -0.38559   -0.68797   -0.41632   -0.13183   -0.24922\n",
      "  0.441      0.085919   0.20871   -0.063582   0.062228  -0.051234\n",
      " -0.13398    1.1418     0.036526   0.49029   -0.24567   -0.412\n",
      "  0.12349    0.41336   -0.48397   -0.54243   -0.27787   -0.26015\n",
      " -0.38485    0.78656    0.1023    -0.20712    0.40751    0.32026\n",
      " -0.51052    0.48362   -0.0099498 -0.38685    0.034975  -0.167\n",
      "  0.4237    -0.54164   -0.30323   -0.36983    0.082836  -0.52538\n",
      " -0.064531  -1.398     -0.14873   -0.35327   -0.1118     1.0912\n",
      "  0.095864  -2.8129     0.45238    0.46213    1.6012    -0.20837\n",
      " -0.27377    0.71197   -1.0754    -0.046974   0.67479   -0.065839\n",
      "  0.75824    0.39405    0.15507   -0.64719    0.32796   -0.031748\n",
      "  0.52899   -0.43886    0.67405    0.42136   -0.11981   -0.21777\n",
      " -0.29756   -0.1351     0.59898    0.46529   -0.58258   -0.02323\n",
      " -1.5442     0.01901   -0.015877   0.024499  -0.58017   -0.67659\n",
      " -0.040379  -0.44043    0.083292   0.20035   -0.75499    0.16918\n",
      " -0.26573   -0.52878    0.17584    1.065    ]\n",
      "word:wonderfully and index:20\n",
      "[-0.56043    0.30893    0.18176   -0.086719   0.52825    0.63574\n",
      " -0.023271   0.48158   -0.27372   -0.0733     0.30217   -0.17707\n",
      "  0.089869  -0.16263   -0.10093    0.33688   -0.35809    0.11526\n",
      "  0.038377   0.4621     0.35182    0.32603    0.21842   -0.84708\n",
      "  0.51889    0.089675  -0.82897    0.11645   -0.47664    0.14107\n",
      " -0.47045    0.18205   -0.25845   -0.5048     0.15755    0.047771\n",
      " -0.12368   -0.037123  -0.0087457 -0.52205    1.1449     0.42038\n",
      " -0.030864   0.18269   -0.19306    0.19216    0.18259    0.47315\n",
      "  0.46634   -0.318     -0.1106    -0.46516    0.67982    0.13072\n",
      " -0.3755    -0.52611    0.57044    0.30823   -0.38756   -0.70315\n",
      "  0.052363   1.335     -0.29171   -0.31765    0.29148   -0.66712\n",
      "  0.66889    0.03414   -0.081772  -0.23806    0.62443    0.10337\n",
      "  0.47184    0.14001    0.76388   -0.14261   -0.43488   -0.32311\n",
      "  0.032953  -0.041793  -0.36843    0.23463   -0.24854    0.29429\n",
      " -1.017     -0.004488   0.5175     0.23836   -1.097     -0.76094\n",
      "  0.096627   0.14751   -0.43459   -0.37262    0.18109    0.0014254\n",
      " -0.69193   -0.76847   -0.73617    0.76595  ]\n",
      "word:directed and index:21\n",
      "[-0.21567   -0.33206   -0.45313   -0.41919    0.82384    0.36837\n",
      " -0.61553    0.16841   -0.62995   -0.023168   0.2699    -0.4657\n",
      "  0.094082   0.26985    0.2795     0.1807     0.0074895  0.29671\n",
      " -0.63218    0.86336    0.42662   -0.55487    0.33863   -0.21184\n",
      "  0.24564    0.40058    0.17919    0.066713   0.098859   0.17272\n",
      " -0.56386    0.5703    -0.14503    0.35375   -0.26583   -0.14769\n",
      "  0.80013   -0.0067112 -0.39313    0.027752  -0.10297    0.30957\n",
      " -1.0458     1.176      0.92512   -0.56637   -0.77477   -0.83083\n",
      " -0.11463    0.055441  -0.40675   -0.054181   0.46651    0.81062\n",
      "  0.033595  -2.1541    -0.36123    0.85944    0.50093   -0.4943\n",
      "  0.4748     1.1931     0.42139   -0.10817    1.0794    -0.50794\n",
      "  0.26591    1.1886     0.21978    1.0539     0.47414   -0.011562\n",
      "  0.026147  -0.031809  -0.16441    0.045222  -0.27503    0.67033\n",
      " -0.59198    0.089545  -0.23214    0.12678   -0.2528     0.063869\n",
      " -1.6515     0.088796  -0.18252   -0.31924   -0.2131    -0.9439\n",
      " -0.56847   -0.75329   -0.61068    0.30506    0.58672    0.40351\n",
      "  0.061258   0.024186  -0.43767   -0.26252  ]\n",
      "word:and and index:22\n",
      "[-0.071953  0.23127   0.023731 -0.50638   0.33923   0.1959   -0.32943\n",
      "  0.18364  -0.18057   0.28963   0.20448  -0.5496    0.27399   0.58327\n",
      "  0.20468  -0.49228   0.19974  -0.070237 -0.88049   0.29485   0.14071\n",
      " -0.1009    0.99449   0.36973   0.44554   0.28998  -0.1376   -0.56365\n",
      " -0.029365 -0.4122   -0.25269   0.63181  -0.44767   0.24363  -0.10813\n",
      "  0.25164   0.46967   0.3755   -0.23613  -0.14129  -0.44537  -0.65737\n",
      " -0.042421 -0.28636  -0.28811   0.063766  0.20281  -0.53542   0.41307\n",
      " -0.59722  -0.38614   0.19389  -0.17809   1.6618   -0.011819 -2.3737\n",
      "  0.058427 -0.2698    1.2823    0.81925  -0.22322   0.72932  -0.053211\n",
      "  0.43507   0.85011  -0.42935   0.92664   0.39051   1.0585   -0.24561\n",
      " -0.18265  -0.5328    0.059518 -0.66019   0.18991   0.28836  -0.2434\n",
      "  0.52784  -0.65762  -0.14081   1.0491    0.5134   -0.23816   0.69895\n",
      " -1.4813   -0.2487   -0.17936  -0.059137 -0.08056  -0.48782   0.014487\n",
      " -0.6259   -0.32367   0.41862  -1.0807    0.46742  -0.49931  -0.71895\n",
      "  0.86894   0.19539 ]\n",
      "word:executed and index:23\n",
      "[ 0.7613     0.17972   -0.76899   -0.2865     0.77247    0.94735\n",
      " -0.07219   -0.081129  -0.57753    0.52815    0.42283    1.2412\n",
      "  0.8625     0.018654   0.15125    0.30509   -0.23245   -0.10358\n",
      " -1.2818     0.010024   0.79532   -0.84856    0.51284    0.08649\n",
      " -0.41817   -0.29105   -0.18412    0.17066    0.52095    0.49295\n",
      "  0.37253    0.13268   -0.34252   -0.43362    0.2928    -0.13918\n",
      " -0.42655   -0.65714   -0.026113  -0.75566   -0.2885    -0.19096\n",
      "  0.49674    0.044491   0.48456   -0.40331    0.36906   -0.6017\n",
      "  0.0047095 -0.19035   -0.38477   -0.33294    0.34378    1.3798\n",
      " -0.77095   -0.254     -0.37923   -0.27679    0.92215   -0.16385\n",
      " -0.52807    0.8619    -0.36652   -0.3593     0.35809   -0.55749\n",
      "  0.13815    0.79798   -0.70326    0.62853   -0.28412   -0.42784\n",
      "  0.19872    0.43947    0.25297    0.065048  -0.27234    0.0071566\n",
      " -0.026621   0.32868    0.94569    0.47178   -0.50544    0.41182\n",
      " -1.1491     0.080121   0.75512   -0.56853   -0.18012   -0.71163\n",
      "  0.15222   -0.85798    0.27506    0.37783   -0.47928   -0.18677\n",
      " -0.23024   -0.64985   -0.27803   -0.93691  ]\n",
      "word:its and index:24\n",
      "[ 0.20589  -0.3171    0.74431   0.047407 -0.10826  -0.18517  -0.47992\n",
      "  0.27736  -0.47515   0.63156   0.035472  0.073577  0.36199  -0.1923\n",
      " -0.43884  -0.32988   0.44746  -0.39097   0.46493   0.12546   0.38224\n",
      " -0.45183   0.29384   0.62979   0.32395  -0.22369  -0.16764  -0.30502\n",
      " -0.10406  -0.27813   0.32921   0.62754  -0.57658   0.12239  -0.56023\n",
      "  0.44731   0.48498  -0.20299  -0.92786  -0.22441   0.21687  -0.26173\n",
      "  0.50944   0.30419   0.5882    0.40474   0.57113  -0.24145  -0.076213\n",
      " -0.61215  -0.016607  0.10699  -0.083368  1.4618   -0.58467  -3.2127\n",
      "  0.41215  -0.79032   2.7901   -0.39825   0.11872   0.58777   0.25648\n",
      "  0.42427   0.63226  -0.1714    0.11396   0.61377   0.65834  -0.21013\n",
      "  0.24094  -0.036859 -0.2394   -0.87062   0.2499   -0.55732  -0.494\n",
      " -0.12043  -1.6417    0.43463   0.29555   0.075822 -0.3107    1.0174\n",
      " -1.115    -0.31493   0.41304  -0.1616    0.070745  0.21949   0.034177\n",
      "  0.78235  -0.39665   0.34179  -0.73382  -0.45151  -0.81274  -0.67858\n",
      "  1.2779    0.86475 ]\n",
      "word:series and index:25\n",
      "[-6.0306e-01 -1.2727e-01  4.4920e-02 -2.6445e-01  2.3855e-01 -5.7307e-02\n",
      "  4.0148e-01  6.5752e-01 -1.2192e+00 -5.4203e-01  3.2482e-01 -4.3304e-01\n",
      "  1.0481e-03  4.2314e-01 -2.9459e-01  6.4495e-01  5.3798e-02  6.1311e-01\n",
      "  1.8401e-01  6.5275e-01  9.0114e-01 -6.6002e-01 -7.7375e-02  3.9401e-01\n",
      "  1.7032e+00 -4.0581e-01  5.5797e-01  3.4985e-01 -2.9196e-01  5.8662e-01\n",
      " -4.4293e-01  3.3279e-01 -6.9469e-01  3.6138e-01 -9.4412e-04  1.9171e-01\n",
      " -1.7079e-01 -6.7731e-01 -7.5038e-01  4.0546e-01 -1.9759e-02  3.2476e-01\n",
      " -1.0229e-01  2.3223e-01  8.8904e-02 -1.1097e-01 -3.3615e-02 -4.0234e-01\n",
      "  3.3279e-01 -3.3548e-02 -9.6708e-01  1.9220e-01  3.1386e-01  1.0695e+00\n",
      "  5.1263e-01 -2.8874e+00  3.3803e-02  6.7321e-01  8.9788e-01  9.0397e-01\n",
      " -9.5694e-01  1.1062e+00 -3.9357e-02 -2.4912e-01  1.6123e+00 -3.5618e-01\n",
      " -7.5157e-02  1.9508e-01 -4.2009e-02  5.8326e-01 -3.0853e-02  1.5260e-02\n",
      " -5.9582e-01 -3.4678e-02 -3.5916e-01  5.4510e-01 -3.6531e-02  4.5404e-01\n",
      " -1.2793e+00  1.4243e-01  3.2617e-01  1.6790e-01 -1.3502e-01  2.9914e-01\n",
      " -1.3607e+00  1.4313e-01 -7.8040e-01 -6.0118e-01 -2.0088e-01  3.7998e-01\n",
      " -4.4130e-01  6.6399e-01 -2.1784e-01  1.6086e-01 -3.2219e-02 -5.2324e-02\n",
      " -3.2160e-01 -3.9676e-02  7.4202e-01  6.1783e-02]\n",
      "word:never and index:26\n",
      "[ 0.28308    0.3328     0.50003   -0.2043    -0.34403    0.10466\n",
      "  0.25852    0.11798    0.50594   -0.22371    0.52499    0.59336\n",
      "  0.52034    0.38505   -0.094194  -0.27289    0.3719     0.71022\n",
      " -0.78723    0.142      0.23843    0.17683   -0.13      -0.51327\n",
      "  0.13746    0.092858  -0.47914   -1.0342     0.51481   -0.4158\n",
      "  0.17622    1.0603     0.038309   0.47409    0.34532    0.10244\n",
      " -0.10949    0.031997  -0.03563   -0.073314  -0.61494   -0.0056377\n",
      "  0.40174   -0.47975   -0.053224  -0.23983    0.64644   -0.48115\n",
      "  0.73126   -1.4097     0.15377   -0.093436   0.42553    1.0389\n",
      "  0.35117   -2.2379    -0.2496     0.24372    0.92944    0.87231\n",
      "  0.24636    0.85973   -0.72904   -0.30587    1.1191    -0.16524\n",
      "  0.65312    0.22118   -0.53921   -0.034287   0.39948   -0.34075\n",
      "  0.048538  -0.023853  -0.076588   0.06669   -0.16751   -0.2614\n",
      " -0.8065     0.36449    0.18566   -0.03655   -0.56927   -0.093071\n",
      " -2.0503    -0.50041   -0.0049648 -0.33624   -0.17233   -0.19766\n",
      " -0.35015   -0.63284    0.12563    0.022786  -0.56203    0.21945\n",
      " -0.4995     0.031549  -0.22464    0.48508  ]\n",
      "word:watched and index:27\n",
      "[ 0.12114    0.3416    -0.064208  -0.33362   -0.72215    0.38678\n",
      "  0.092936   0.40416   -0.44754   -0.63221    0.086218   0.11538\n",
      "  0.16914   -0.96135   -0.054605  -0.051946   0.14388    0.64705\n",
      " -0.10599   -0.40242    0.23345    0.12296    0.31217   -0.016966\n",
      "  0.77669   -0.95044    0.057955   0.56492    0.4512     0.28238\n",
      "  0.29521    0.14449    0.23913    0.9046     0.4625     0.06442\n",
      " -0.058131   0.18484    0.45894    0.10577   -0.38371   -0.54825\n",
      " -0.053757   0.45803    0.033931   0.0099511 -0.15368    0.01096\n",
      "  0.25259   -1.3269    -0.050905  -0.57506    0.52022    0.78347\n",
      "  0.14718   -1.6214    -0.49741    0.18722    0.66042    0.50621\n",
      "  0.092716   1.1395     0.13036   -0.59942   -0.12388    0.43274\n",
      "  0.29957    0.39537    0.11601    0.97524    0.32525    0.026772\n",
      "  0.30992   -0.12346   -0.40302    0.6261     0.020524   0.025914\n",
      "  0.2904    -0.161      0.75674    0.50355   -0.29333   -0.033565\n",
      " -1.3157    -1.4105    -0.46418    0.14765   -0.062277  -0.31416\n",
      "  0.50445   -0.35064   -0.17486   -0.3988    -0.85518   -0.14531\n",
      "  0.24262    0.52999    0.18912    0.42742  ]\n",
      "word:such and index:28\n",
      "[-0.55579    0.411     -0.056376  -0.17861    0.3167    -0.29112\n",
      " -0.95892   -0.27791   -0.45842   -0.054586  -0.49591   -0.77259\n",
      "  0.13252    0.32002    0.17383   -0.23332    0.1501     0.3166\n",
      " -0.32644    0.70956    0.076804  -0.078303   0.16906    0.042042\n",
      "  0.038783  -0.01848    0.13735   -0.44243   -0.44468   -0.11372\n",
      "  0.13618    0.024879  -0.62403   -0.087121   0.45986    0.50535\n",
      "  0.47502   -0.026935  -0.5409    -0.41838   -0.11559    0.1777\n",
      " -0.092615  -0.36311   -0.04453    0.015722   0.28546   -0.14785\n",
      " -0.58193   -0.56128    0.6025     0.79511   -0.031751   0.69088\n",
      "  0.075027  -2.1401     0.38418   -0.15997    1.483      0.68202\n",
      " -0.7515     1.1908     0.11901    0.3035     1.1821    -0.15734\n",
      "  0.8468    -0.0025797  0.77233   -0.54517   -0.7382    -0.48891\n",
      " -0.22168   -0.2713    -0.1845     0.04291    0.13681    0.053616\n",
      " -0.92298    0.01167    1.2265    -0.36393   -0.22368    0.58218\n",
      " -2.2609     0.23546   -0.052839  -0.32235   -0.14765   -0.73103\n",
      " -0.28457   -0.4235    -0.37963    0.05226   -0.46462   -0.30236\n",
      " -0.70037   -0.7378     1.1683     0.11592  ]\n",
      "word:brillent and index:29\n",
      "None\n",
      "word:wonderful and index:30\n",
      "[ 0.19742    0.5114     0.19043   -0.20615   -0.23031    0.25671\n",
      "  0.22668    0.52012   -0.37792   -0.48753    0.10269   -0.40277\n",
      " -0.1782    -0.31256   -0.02286    0.336     -0.051616   0.29896\n",
      " -0.35625    1.0729    -0.20798    0.43333   -0.34719   -0.76778\n",
      "  0.2825     0.51155   -0.22812    0.22737   -0.21706   -0.33989\n",
      " -0.52549    0.59313    0.19105   -0.55131    0.092535   0.008514\n",
      " -0.40322    0.46742    0.41892   -0.80948    0.28911    0.20081\n",
      "  0.38232   -0.31885    0.27433    0.25773    0.17946   -0.072586\n",
      "  0.08651   -0.70698   -0.1399    -0.34831    0.55074    0.28158\n",
      " -0.1381    -1.8367     0.43073    0.73006    0.39284   -0.44575\n",
      " -0.11662    1.4685    -0.66575   -0.39895    0.45373   -0.3806\n",
      "  0.68791    0.033634   0.052634  -0.74561    0.36944    0.13868\n",
      "  0.41492   -0.13675    0.6654     0.84479    0.094823  -0.41032\n",
      "  0.26674    0.28715   -0.11217    0.18606    0.54309    0.2368\n",
      " -1.1154     0.14318    0.1404    -0.31462   -0.54193   -0.65945\n",
      "  0.28       0.0023144  0.44542   -0.30454   -0.8038    -0.22728\n",
      " -0.6984    -0.7385    -0.35548    1.1172   ]\n"
     ]
    }
   ],
   "source": [
    "# Creating word embedding for our Corpus\n",
    "\n",
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "\n",
    "for word,index in word_tokenizer.word_index.items():\n",
    "    print(\"word:{0} and index:{1}\".format(word,index))\n",
    "    print(embeddings_dictionary.get(word))\n",
    "    \n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 100)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n",
    "\n",
    "# 31 : My corpus Vocabulary\n",
    "# 100 : Dimention of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word2Vec Using gensim Model\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "norm_bible = ['beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters']\n",
    "\n",
    "norm_bible = [x.split() for x in norm_bible]\n",
    "\n",
    "norm_bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beginning',\n",
       " 'created',\n",
       " 'darkness',\n",
       " 'deep',\n",
       " 'earth',\n",
       " 'face',\n",
       " 'form',\n",
       " 'god',\n",
       " 'heaven',\n",
       " 'moved',\n",
       " 'spirit',\n",
       " 'upon',\n",
       " 'void',\n",
       " 'waters',\n",
       " 'without'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary\n",
    "_vocab = set([x for y in norm_bible for x in y])\n",
    "\n",
    "_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocab size\n",
    "len(_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Word2Vec \n",
    "\n",
    "model = Word2Vec(sentences=norm_bible, max_vocab_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you save the model you can continue training it later:\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'beginning': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9bc8>,\n",
       " 'god': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9c88>,\n",
       " 'created': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9cc8>,\n",
       " 'heaven': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9d08>,\n",
       " 'earth': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9dc8>,\n",
       " 'without': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9d88>,\n",
       " 'form': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9e08>,\n",
       " 'void': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9e48>,\n",
       " 'darkness': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9c48>,\n",
       " 'upon': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9d48>,\n",
       " 'face': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9e88>,\n",
       " 'deep': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9ec8>,\n",
       " 'spirit': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9f08>,\n",
       " 'moved': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9f48>,\n",
       " 'waters': <gensim.models.keyedvectors.Vocab at 0x19b2c8a9f88>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0049982 , -0.00495543,  0.00209568, -0.00377834, -0.0047039 ,\n",
       "        0.00377543,  0.00446357,  0.00402102, -0.00120432,  0.00443419,\n",
       "       -0.00336269, -0.00046212, -0.00294839,  0.00034047,  0.00109328,\n",
       "        0.00097902, -0.00194222,  0.00453343,  0.00420529, -0.00413985,\n",
       "       -0.00026492, -0.00105644, -0.00088214, -0.00387856, -0.00107633,\n",
       "        0.00390099,  0.00364199, -0.00235461, -0.00188958, -0.00055888,\n",
       "       -0.00179535, -0.00260311, -0.00431528, -0.00444491, -0.00311525,\n",
       "       -0.00442708,  0.00059373, -0.00175032,  0.0004481 ,  0.00342633,\n",
       "        0.00362875, -0.00408895,  0.00233011, -0.00074572,  0.00259224,\n",
       "        0.00111378,  0.00019834, -0.0044388 ,  0.00204787, -0.00280272,\n",
       "       -0.00190209, -0.00457306,  0.00271019, -0.00094671,  0.001092  ,\n",
       "        0.00111628,  0.0043809 ,  0.00190675,  0.00205206,  0.00484966,\n",
       "       -0.00032888, -0.00152577, -0.00419053,  0.00498315, -0.00337296,\n",
       "        0.00291396, -0.00077983,  0.004306  ,  0.00166933,  0.00031806,\n",
       "       -0.000135  ,  0.0039076 , -0.00474982, -0.0044731 , -0.00117256,\n",
       "        0.0003146 , -0.00343653, -0.00237088, -0.00262288, -0.00372765,\n",
       "       -0.00046939,  0.00277891, -0.00468581,  0.00221245,  0.00351948,\n",
       "        0.00065363,  0.00419406, -0.00338332,  0.00148746,  0.00126174,\n",
       "       -0.00194454, -0.00398086,  0.00143983, -0.00152718, -0.00309812,\n",
       "       -0.00304217,  0.00353361, -0.00238928,  0.00431747,  0.00223043],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulding the vector : Numpy array for the word \n",
    "\n",
    "model.wv['god']  # get numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.9981982e-03, -4.9554287e-03,  2.0956798e-03, -3.7783368e-03,\n",
       "        -4.7039031e-03,  3.7754276e-03,  4.4635679e-03,  4.0210173e-03,\n",
       "        -1.2043229e-03,  4.4341907e-03, -3.3626903e-03, -4.6211795e-04,\n",
       "        -2.9483901e-03,  3.4047215e-04,  1.0932811e-03,  9.7901549e-04,\n",
       "        -1.9422161e-03,  4.5334264e-03,  4.2052898e-03, -4.1398495e-03,\n",
       "        -2.6492067e-04, -1.0564369e-03, -8.8213559e-04, -3.8785576e-03,\n",
       "        -1.0763302e-03,  3.9009862e-03,  3.6419891e-03, -2.3546142e-03,\n",
       "        -1.8895830e-03, -5.5888243e-04, -1.7953550e-03, -2.6031071e-03,\n",
       "        -4.3152752e-03, -4.4449074e-03, -3.1152535e-03, -4.4270777e-03,\n",
       "         5.9372769e-04, -1.7503235e-03,  4.4810280e-04,  3.4263306e-03,\n",
       "         3.6287452e-03, -4.0889466e-03,  2.3301095e-03, -7.4572384e-04,\n",
       "         2.5922395e-03,  1.1137846e-03,  1.9833964e-04, -4.4387956e-03,\n",
       "         2.0478657e-03, -2.8027203e-03, -1.9020891e-03, -4.5730560e-03,\n",
       "         2.7101890e-03, -9.4670954e-04,  1.0919985e-03,  1.1162766e-03,\n",
       "         4.3808995e-03,  1.9067504e-03,  2.0520561e-03,  4.8496639e-03,\n",
       "        -3.2887704e-04, -1.5257727e-03, -4.1905330e-03,  4.9831490e-03,\n",
       "        -3.3729561e-03,  2.9139617e-03, -7.7983056e-04,  4.3059965e-03,\n",
       "         1.6693256e-03,  3.1805926e-04, -1.3499889e-04,  3.9076041e-03,\n",
       "        -4.7498201e-03, -4.4731046e-03, -1.1725588e-03,  3.1460327e-04,\n",
       "        -3.4365270e-03, -2.3708777e-03, -2.6228782e-03, -3.7276507e-03,\n",
       "        -4.6939170e-04,  2.7789068e-03, -4.6858070e-03,  2.2124473e-03,\n",
       "         3.5194841e-03,  6.5362948e-04,  4.1940613e-03, -3.3833177e-03,\n",
       "         1.4874619e-03,  1.2617398e-03, -1.9445361e-03, -3.9808615e-03,\n",
       "         1.4398308e-03, -1.5271778e-03, -3.0981242e-03, -3.0421680e-03,\n",
       "         3.5336148e-03, -2.3892815e-03,  4.3174662e-03,  2.2304251e-03],\n",
       "       [ 3.0283716e-03,  2.2358140e-03,  3.7334525e-04, -4.3200227e-03,\n",
       "         3.7001623e-03, -4.2773420e-03, -4.4012084e-03, -1.6640992e-03,\n",
       "        -1.7264520e-03,  2.2508826e-03,  2.7121031e-03,  3.4042983e-04,\n",
       "         2.0152356e-03, -3.0306550e-03,  3.3836742e-03, -2.9872314e-03,\n",
       "         4.2329212e-03, -4.7643835e-04,  9.2791638e-04,  3.9160494e-03,\n",
       "        -4.7707646e-03,  3.7788341e-03,  2.6874691e-03, -6.2671863e-04,\n",
       "         2.0187080e-03,  1.7366555e-04, -4.8853789e-04,  2.7611442e-03,\n",
       "         4.0709614e-03,  3.6211303e-04,  1.6455263e-03, -4.1896626e-03,\n",
       "         4.0286882e-03, -1.6757065e-03, -2.7619000e-03,  4.9557202e-03,\n",
       "         2.7121087e-03,  1.0129787e-03, -2.9628265e-03,  3.0778293e-03,\n",
       "         4.3930747e-03,  3.9432389e-03,  6.1663426e-04,  1.8835162e-03,\n",
       "         1.2960579e-03, -1.7364050e-03,  5.8039819e-04,  7.4024987e-04,\n",
       "        -2.5559533e-03, -1.7573962e-03, -3.5537055e-03,  4.8578084e-03,\n",
       "         3.3195519e-03,  2.1502648e-03, -3.0928256e-03,  5.2650172e-05,\n",
       "         2.1806378e-03,  1.7941692e-03,  4.1442835e-03,  1.9645721e-03,\n",
       "        -2.4037440e-03, -9.9098333e-04,  4.3084258e-03,  1.8493725e-03,\n",
       "        -4.0785698e-03,  3.4946942e-04, -6.2475266e-04,  2.6251029e-03,\n",
       "         3.9768656e-04, -3.3575040e-03, -3.6079667e-03,  1.9719689e-04,\n",
       "        -3.9813123e-03,  4.3715048e-03,  5.0258968e-04, -2.5506436e-03,\n",
       "         2.4480957e-03, -4.3774629e-03,  4.9964753e-03,  1.4279474e-04,\n",
       "         2.0381561e-03, -2.4118919e-03, -1.7165106e-03, -3.0002913e-03,\n",
       "        -7.0966140e-05, -2.2061730e-03, -2.7636536e-03, -2.1899634e-03,\n",
       "         4.2185434e-03, -3.1257467e-03, -2.7128810e-03, -3.5518373e-04,\n",
       "         4.7183577e-03,  3.1508210e-03,  2.0242338e-03,  2.2384764e-03,\n",
       "        -4.9358201e-03,  1.1896866e-03,  1.1996763e-03,  4.6806483e-04]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['god','heaven']  # get numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('moved', 0.14142993092536926),\n",
       " ('darkness', 0.13165391981601715),\n",
       " ('spirit', 0.1094588041305542),\n",
       " ('without', 0.09498775750398636),\n",
       " ('form', 0.03891271352767944),\n",
       " ('beginning', 0.01158369705080986),\n",
       " ('earth', -0.044981859624385834),\n",
       " ('face', -0.06618501245975494),\n",
       " ('void', -0.0918918028473854),\n",
       " ('deep', -0.10328714549541473)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get other similar words\n",
    "\n",
    "sims = model.wv.most_similar('god', topn=10)  \n",
    "\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between Word2Vec and Doc2Vec\n",
    "# 1. As the names suggest Word2Vec is trained on single words while Doc2vec is trained texts of variable length,\n",
    "\n",
    "# Glove and word2vec, these are unsupervised approaches for building vectors from text data.\n",
    "# The main difference is the functioning of both models. Glove is Count based model, and Word2vec is predictive modeling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
