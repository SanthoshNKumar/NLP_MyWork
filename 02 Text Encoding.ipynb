{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'is', 'quick', 'and', 'he', 'is', 'jumping', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "#  Text to Word : Split\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "words = sentence.split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Length of the senetence\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max length of the word in document\n",
    "np.max([len(x) for x in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'is', 'brown', 'the', 'he', 'over', 'jumping', 'lazy', 'dog', 'and', 'quick', 'fox'}\n",
      "Vocabulary size: 11\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary List\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = \"The brown fox is quick and he is jumping over the lazy dog\"\n",
    "\n",
    "vocab = set(text_to_word_sequence(sentence))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(\"Vocabulary:\",vocab)\n",
    "print(\"Vocabulary size:\",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['king', 'james', 'bible'],\n",
       " ['old', 'testament', 'king', 'james', 'bible'],\n",
       " ['first', 'book', 'moses', 'called', 'genesis'],\n",
       " ['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters'],\n",
       " ['god', 'said', 'let', 'light', 'light'],\n",
       " ['god', 'saw', 'light', 'good', 'god', 'divided', 'light', 'darkness'],\n",
       " ['god', 'called', 'light', 'day', 'darkness', 'called', 'night'],\n",
       " ['evening', 'morning', 'first', 'day']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to word Sequence\n",
    "\n",
    "from keras.preprocessing.text import text\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "[[w for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'brown': 1, 'fox': 1, 'is': 2, 'quick': 1, 'and': 1, 'he': 1, 'jumping': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "# Word Frequency (Word Count)\n",
    "\n",
    "word_freq = {}\n",
    "\n",
    "for tok in sentence.split():\n",
    "    if tok in word_freq:\n",
    "        word_freq[tok] +=1\n",
    "    else:\n",
    "        word_freq[tok] = 1\n",
    "\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 2, 'The': 1, 'brown': 1, 'fox': 1, 'quick': 1, 'and': 1, 'he': 1, 'jumping': 1, 'over': 1, 'the': 1, 'lazy': 1, 'dog': 1})\n"
     ]
    }
   ],
   "source": [
    "# Word Frequency (Word Count)\n",
    "\n",
    "import collections\n",
    "\n",
    "counter = collections.Counter(sentence.split())\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 2),\n",
       " ('The', 1),\n",
       " ('brown', 1),\n",
       " ('fox', 1),\n",
       " ('quick', 1),\n",
       " ('and', 1),\n",
       " ('he', 1),\n",
       " ('jumping', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Most Common word\n",
    "counter.most_common(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 0, 'brown': 1, 'The': 2, 'the': 3, 'he': 4, 'over': 5, 'jumping': 6, 'lazy': 7, 'dog': 8, 'and': 9, 'quick': 10, 'fox': 11}\n"
     ]
    }
   ],
   "source": [
    "# Word Index\n",
    "\n",
    "corpus = sentence.split()\n",
    "\n",
    "uniq_text = set(corpus)\n",
    "\n",
    "text_to_int = {}\n",
    "\n",
    "for i, c in enumerate (uniq_text):\n",
    "    text_to_int.update({c: i})\n",
    "    \n",
    "print(text_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is', 'brown', 'the', 'he', 'over', 'jumping', 'lazy', 'dog', 'and', 'quick', 'fox'}\n"
     ]
    }
   ],
   "source": [
    "# Unique words in Senetnce\n",
    "\n",
    "print(set(sentence.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'brown'],\n",
       " ['brown', 'fox'],\n",
       " ['fox', 'is'],\n",
       " ['is', 'quick'],\n",
       " ['quick', 'and'],\n",
       " ['and', 'he'],\n",
       " ['he', 'is'],\n",
       " ['is', 'jumping'],\n",
       " ['jumping', 'over'],\n",
       " ['over', 'the'],\n",
       " ['the', 'lazy'],\n",
       " ['lazy', 'dog']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram (n =2): Bigram\n",
    "\n",
    "words =sentence.split()\n",
    "\n",
    "n = 2\n",
    "output= []\n",
    "for i in range(len(words) - n+1):\n",
    "    output.append(words[i:i+n])\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'brown', 'fox'],\n",
       " ['brown', 'fox', 'is'],\n",
       " ['fox', 'is', 'quick'],\n",
       " ['is', 'quick', 'and'],\n",
       " ['quick', 'and', 'he'],\n",
       " ['and', 'he', 'is'],\n",
       " ['he', 'is', 'jumping'],\n",
       " ['is', 'jumping', 'over'],\n",
       " ['jumping', 'over', 'the'],\n",
       " ['over', 'the', 'lazy'],\n",
       " ['the', 'lazy', 'dog']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram (n =3): Trigram\n",
    "\n",
    "words =sentence.split()\n",
    "\n",
    "n = 3\n",
    "output= []\n",
    "for i in range(len(words) - n+1):\n",
    "    output.append(words[i:i+n])\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Located': 1,\n",
       " 'on': 2,\n",
       " 'the': 9,\n",
       " 'southern': 4,\n",
       " 'tip': 5,\n",
       " 'of': 6,\n",
       " 'Lake': 7,\n",
       " 'Union,': 8,\n",
       " 'Hilton': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to Integer\n",
    "\n",
    "list1 = ['Located', 'on', 'the', 'southern', 'tip', 'of', 'Lake', 'Union,', 'the', 'Hilton']\n",
    "\n",
    "text_int ={}\n",
    "\n",
    "for i,j in enumerate(list1):\n",
    "    text_int.update({j:(i+1)})\n",
    "    \n",
    "text_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 6, 7],\n",
       " [13, 14, 5, 6, 7],\n",
       " [8, 15, 16, 3, 17],\n",
       " [18, 1, 19, 20, 9],\n",
       " [9, 21, 22, 23, 4, 10, 11, 24],\n",
       " [25, 1, 26, 10, 11, 27],\n",
       " [1, 28, 29, 2, 2],\n",
       " [1, 30, 2, 31, 1, 32, 2, 4],\n",
       " [1, 3, 2, 12, 4, 3, 33],\n",
       " [34, 35, 8, 12]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to Integer\n",
    "\n",
    "from keras.preprocessing import text\n",
    "\n",
    "norm_bible = ['king james bible',\n",
    "             'old testament king james bible',\n",
    "             'first book moses called genesis',\n",
    "             'beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters',\n",
    "             'god said let light light',\n",
    "             'god saw light good god divided light darkness',\n",
    "             'god called light day darkness called night',\n",
    "             'evening morning first day']\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary One Hot Encoder for One Senetence\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Bag of Word\n",
    "\n",
    "# One Hot Encoding\n",
    "\n",
    "doc = \"Can I eat the Pizza\".lower().split()\n",
    "\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(doc)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(doc, mode='count')\n",
    "\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Ending for Multiple Sentence\n",
    "\n",
    "doc = [['can', 'i', 'eat', 'the', 'pizza'],['can', 'i', 'eat', 'the', 'pizza']]\n",
    "\n",
    "\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(doc)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(doc, mode='count')\n",
    "\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding : Binary\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# define 5 documents\n",
    "\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!']\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "\n",
    "print(encoded_docs)\n",
    "\n",
    "# Word Index\n",
    "\n",
    "#'work': 1,\n",
    "# 'well': 2,\n",
    "# 'done': 3,\n",
    "# 'good': 4,\n",
    "# 'great': 5,\n",
    "# 'effort': 6,\n",
    "# 'nice': 7,\n",
    "# 'excellent': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi Dimenstional One Hot Encosing\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "x = [[1,2,3,4,5],\n",
    "     [1,4,8,4,4],\n",
    "     [1,2,3,3,5]]\n",
    "\n",
    "y = to_categorical(x, num_classes=10)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "doc =  ['S','C','Q','S','C','Q','S','C','Q','Q','C','Q','C','Q','Q']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "label_encoder.fit(['S','C','Q'])\n",
    "\n",
    "label_encoder.transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [2.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OrdinalEncoder converts each string value to a whole number. \n",
    "# The first unique value in your column becomes 1, the second becomes 2, the third becomes 3, and so on.\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "doc =  [['S'],['C'],['Q'],['S'],['C'],['Q'],['S'],['C'],['Q'],['Q'],['C'],['Q'],['C'],['Q'],['Q']]\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "encoder.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0.],\n",
       "       [2., 3.],\n",
       "       [1., 1.],\n",
       "       [0., 2.],\n",
       "       [2., 3.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OrdinalEncoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[\"good\", \"london\"],\n",
    "             [\"good\", \"tokyo\"],\n",
    "             [\"bad\", \"paris\"],\n",
    "             [\"average\", \"so so\"],\n",
    "             [\"good\", \"tokyo\"]])\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "encoder.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 2, 1]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = np.array(['The sun is shining',\n",
    "                 'The weather is sweet',\n",
    "                 'The sun is shining and the weather is sweet'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "bag.toarray()\n",
    "\n",
    "# Index\n",
    "\n",
    "# and = 0\n",
    "# is =1\n",
    "# shining = 2\n",
    "# sun =3\n",
    "# sweet =4\n",
    "# the =5\n",
    "# wether =6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>arts</th>\n",
       "      <th>at</th>\n",
       "      <th>becomes</th>\n",
       "      <th>between</th>\n",
       "      <th>both</th>\n",
       "      <th>brained</th>\n",
       "      <th>data</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>left</th>\n",
       "      <th>natural</th>\n",
       "      <th>of</th>\n",
       "      <th>overlap</th>\n",
       "      <th>part</th>\n",
       "      <th>processing</th>\n",
       "      <th>right</th>\n",
       "      <th>science</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.257439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.317989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420947</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>0.159139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249322</td>\n",
       "      <td>0.130107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224449</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183502</td>\n",
       "      <td>0.351643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.308872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.391765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204439</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         an       and       are      arts        at   becomes   between  \\\n",
       "0  0.403328  0.257439  0.000000  0.257439  0.000000  0.000000  0.403328   \n",
       "1  0.000000  0.159139  0.498644  0.159139  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.224449  0.000000  0.224449  0.351643  0.351643  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       both   brained      data  ...  language      left   natural        of  \\\n",
       "0  0.000000  0.000000  0.317989  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.498644  0.000000  ...  0.000000  0.249322  0.000000  0.000000   \n",
       "2  0.351643  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.308872  ...  0.391765  0.000000  0.391765  0.391765   \n",
       "\n",
       "    overlap      part  processing     right   science      time  \n",
       "0  0.403328  0.000000    0.000000  0.000000  0.420947  0.000000  \n",
       "1  0.000000  0.000000    0.000000  0.249322  0.130107  0.000000  \n",
       "2  0.000000  0.000000    0.000000  0.000000  0.183502  0.351643  \n",
       "3  0.000000  0.391765    0.391765  0.000000  0.204439  0.000000  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF\n",
    "\n",
    "# tf-idf = tf(i,d) * idf(t,d)\n",
    "\n",
    "# td(i,d) = term frequency\n",
    "\n",
    "# idf(t,d) = \tlog(n/1+df(t,d)) :  inverse document frequency  df(t,d) : no. of documents d that contain the term t\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\"Data Science is an overlap between Arts and Science\",\n",
    "          \"Generally,Arts graduates are right-brained and Science graduates are left-brained\",\n",
    "          \"Excelling in both Arts and Science at a time becomes difficult\",\n",
    "          \"Natural Language Processing is a part of Data Science\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_model = TfidfVectorizer()\n",
    "features = tfidf_model.fit_transform(corpus)\n",
    "df = pd.DataFrame(features.todense(),columns= sorted(tfidf_model.vocabulary_))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence from List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,   91,    1,  645, 1253,  927],\n",
       "       [   0,   73,    8, 3215,   55,  927],\n",
       "       [   0,    0,    0,  711,  632,   71]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad Sequence (Pre)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "raw_inputs = [[83, 91, 1, 645, 1253, 927],\n",
    "              [73, 8, 3215, 55, 927],\n",
    "              [711, 632, 71]]\n",
    "\n",
    "padded_inputs = pad_sequences(raw_inputs,padding='pre')\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  83,   91,    1,  645, 1253,  927],\n",
       "       [  73,    8, 3215,   55,  927,    0],\n",
       "       [ 711,  632,   71,    0,    0,    0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad Sequence (Post)\n",
    "\n",
    "raw_inputs = [[83, 91, 1, 645, 1253, 927],\n",
    "              [73, 8, 3215, 55, 927],\n",
    "              [711, 632, 71]]\n",
    "\n",
    "padded_inputs = pad_sequences(raw_inputs,padding='post')\n",
    "\n",
    "padded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[24],\n",
    "     [24, 34],\n",
    "     [24, 34, 1],\n",
    "     [24, 34, 1, 9],\n",
    "     [24, 34, 1, 9, 56],\n",
    "     [24, 34, 1, 9, 56, 76],\n",
    "     [24, 34, 1, 9, 56, 76, 90],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67, 54],\n",
    "     [24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]]\n",
    "\n",
    "import numpy as np\n",
    "np.max([len(x) for x in a])\n",
    "\n",
    "row = 11\n",
    "column = len(a)\n",
    "\n",
    "tensor = np.zeros((row,column))\n",
    "\n",
    "for i in range(len(a)):\n",
    "    lst = a[i]\n",
    "    for j in range(len(lst)):\n",
    "        tensor[i][j] = lst[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24]\n",
      "[24, 34]\n",
      "[24, 34, 1]\n",
      "[24, 34, 1, 9]\n",
      "[24, 34, 1, 9, 56]\n",
      "[24, 34, 1, 9, 56, 76]\n",
      "[24, 34, 1, 9, 56, 76, 90]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54]\n",
      "[24, 34, 1, 9, 56, 76, 90, 11, 67, 54, 14]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56,  0,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76,  0,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90,  0,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11,  0,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67,  0,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54,  0],\n",
       "       [24, 34,  1,  9, 56, 76, 90, 11, 67, 54, 14]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N Gram from Sequence List\n",
    "\n",
    "lst = [24,34,1,9,56,76,90,11,67,54,14]\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(0,len(lst)):\n",
    "    print(lst[:i+1])\n",
    "    data.append(lst[:i+1])\n",
    "\n",
    "# Padding\n",
    "pad_sequences(data,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 20]\n",
      " [ 1 12]\n",
      " [ 8  1]\n",
      " [ 9 14]\n",
      " [ 8  1]\n",
      " [ 9  3]]\n"
     ]
    }
   ],
   "source": [
    "# Truncating Sequence (Pre)\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "data_vec = [['3', '18', '9', '3', '11', '5', '20'],\n",
    "            ['3', '8', '1', '12'],\n",
    "            ['18', '1', '8', '1'],\n",
    "            ['8', '1', '9', '14'],\n",
    "            ['25', '1', '8', '1'],\n",
    "            ['9','3']]\n",
    "\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(data_vec, maxlen=2)\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3 18]\n",
      " [ 3  8]\n",
      " [18  1]\n",
      " [ 8  1]\n",
      " [25  1]\n",
      " [ 9  3]]\n"
     ]
    }
   ],
   "source": [
    "# Truncating Sequence (Post)\n",
    "\n",
    "# truncate sequence\n",
    "truncated= pad_sequences(data_vec, maxlen=2,truncating='post')\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union {'are', 'You', 'Hello', 'about', 'Hi', 'am', 'fine', 'How', 'I'}\n"
     ]
    }
   ],
   "source": [
    "# Union\n",
    "\n",
    "document1 = ['Hi', 'How','are','You', 'I','am']\n",
    "document2 = ['Hello','I','am','fine','How','about','You']\n",
    "\n",
    "#Union : words from two documents\n",
    "print(\"Union\",set(document1) | set(document2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection: {'You', 'am', 'How', 'I'}\n"
     ]
    }
   ],
   "source": [
    "# Intersection\n",
    "\n",
    "document1 = ['Hi', 'How','are','You', 'I','am']\n",
    "document2 = ['Hello','I','am','fine','How','about','You']\n",
    "\n",
    "#Intersection : common words across two documents\n",
    "print(\"Intersection:\",set(document1) & set(document2))       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequence to Matrix (mode = binary and num_words = 10)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequences to Matrix (mode = count and num_words = 10)\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='count')\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5         6         7         8    9\n",
       "0  0.0  0.4  0.2  0.2  0.2  0.0  0.000000  0.000000  0.000000  0.0\n",
       "1  0.0  0.0  0.0  0.0  0.5  0.5  0.000000  0.000000  0.000000  0.0\n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.333333  0.333333  0.333333  0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequences to Matrix (mode = freq and num_words = 10)\n",
    "tokenizer = Tokenizer(num_words=10)\n",
    "\n",
    "x_train = [[1,2,3,4,1],\n",
    "           [4,5,],\n",
    "           [6,7,8]]\n",
    "\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='freq')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_letters = abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;''\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \".,;''\"\n",
    "\n",
    "print(\"all_letters = {0}\".format(all_letters))\n",
    "\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genrate Input for the RNN\n",
    "import numpy as np\n",
    "\n",
    "def name_intensor(name):\n",
    "    name_in_tensor = np.zeros((len(name),1,n_letters))\n",
    "    \n",
    "    for i,letter in enumerate(name):\n",
    "        name_in_tensor[i][0][all_letters.find(letter)] = 1\n",
    "    \n",
    "    return name_in_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(1, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "print(name_intensor('a'))\n",
    "print(name_intensor('a').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(2, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "print(name_intensor('af'))\n",
    "print(name_intensor('af').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(5, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "print(name_intensor('anand'))\n",
    "print(name_intensor('anand').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['beginning', 'god', 'created', 'heaven', 'earth'],\n",
       " ['earth', 'without', 'form', 'void', 'darkness', 'upon', 'face', 'deep'],\n",
       " ['spirit', 'god', 'moved', 'upon', 'face', 'waters']]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CBOW\n",
    "\n",
    "from keras.preprocessing import text\n",
    "\n",
    "norm_bible = ['beginning god created heaven earth',\n",
    "             'earth without form void darkness upon face deep',\n",
    "             'spirit god moved upon face waters']\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "wids = [[w for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 1, 6, 7, 2], [2, 8, 9, 10, 11, 3, 4, 12], [13, 1, 14, 3, 4, 15]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "word2id = tokenizer.word_index\n",
    "\n",
    "word2id\n",
    "\n",
    "# build vocabulary of unique words\n",
    "\n",
    "word2id['PAD'] = 0\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "\n",
    "wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(word2id)\n",
    "embed_size = 100\n",
    "window_size = 2 # context window size\n",
    "\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 6, 7, 2]\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '5'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "Input: [1, 6]\n",
      "Output: 5\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 6]\n",
      "context_words1 : [[1, 6]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '1'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '5'\n",
      "Word at range index at 0 is '5'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "Input: [5, 6, 7]\n",
      "Output: 1\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [5, 6, 7]\n",
      "context_words1 : [[5, 6, 7]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '6'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '5'\n",
      "Word at range index at 0 is '5'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 4 is '2'\n",
      "Input: [5, 1, 7, 2]\n",
      "Output: 6\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [5, 1, 7, 2]\n",
      "context_words1 : [[5, 1, 7, 2]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '7'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 4 is '2'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [1, 6, 2]\n",
      "Output: 7\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 6, 2]\n",
      "context_words1 : [[1, 6, 2]]\n",
      "*****************************\n",
      "sentence_length: 5\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '2'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '6'\n",
      "Word at range index at 2 is '6'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '7'\n",
      "Word at range index at 3 is '7'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [6, 7]\n",
      "Output: 2\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [6, 7]\n",
      "context_words1 : [[6, 7]]\n",
      "*****************************\n",
      "###############################################################################\n",
      "[2, 8, 9, 10, 11, 3, 4, 12]\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '2'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "Input: [8, 9]\n",
      "Output: 2\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [8, 9]\n",
      "context_words1 : [[8, 9]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '8'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 0 is '2'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "Input: [2, 9, 10]\n",
      "Output: 8\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [2, 9, 10]\n",
      "context_words1 : [[2, 9, 10]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '9'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '2'\n",
      "Word at range index at 0 is '2'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "Input: [2, 8, 10, 11]\n",
      "Output: 9\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [2, 8, 10, 11]\n",
      "context_words1 : [[2, 8, 10, 11]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '10'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '8'\n",
      "Word at range index at 1 is '8'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "Input: [8, 9, 11, 3]\n",
      "Output: 10\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [8, 9, 11, 3]\n",
      "context_words1 : [[8, 9, 11, 3]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '11'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '9'\n",
      "Word at range index at 2 is '9'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "Input: [9, 10, 3, 4]\n",
      "Output: 11\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [9, 10, 3, 4]\n",
      "context_words1 : [[9, 10, 3, 4]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 5\n",
      "Word at index 5 : '3'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 3\n",
      "end: 8\n",
      "Choosing context word based on condition:\n",
      "[3, 4, 5, 6, 7]\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '10'\n",
      "Word at range index at 3 is '10'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '12'\n",
      "Word at range index at 7 is '12'\n",
      "Input: [10, 11, 4, 12]\n",
      "Output: 3\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [10, 11, 4, 12]\n",
      "context_words1 : [[10, 11, 4, 12]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 6\n",
      "Word at index 6 : '4'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 4\n",
      "end: 9\n",
      "Choosing context word based on condition:\n",
      "[4, 5, 6, 7, 8]\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '11'\n",
      "Word at range index at 4 is '11'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '12'\n",
      "Word at range index at 7 is '12'\n",
      "range index: 8\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [11, 3, 12]\n",
      "Output: 4\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [11, 3, 12]\n",
      "context_words1 : [[11, 3, 12]]\n",
      "*****************************\n",
      "sentence_length: 8\n",
      "WindowSize: 2\n",
      "index: 7\n",
      "Word at index 7 : '12'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 5\n",
      "end: 10\n",
      "Choosing context word based on condition:\n",
      "[5, 6, 7, 8, 9]\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 5 is '3'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 6 is '4'\n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 8\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 9\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [3, 4]\n",
      "Output: 12\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [3, 4]\n",
      "context_words1 : [[3, 4]]\n",
      "*****************************\n",
      "###############################################################################\n",
      "[13, 1, 14, 3, 4, 15]\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 0\n",
      "Word at index 0 : '13'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -2\n",
      "end: 3\n",
      "Choosing context word based on condition:\n",
      "[-2, -1, 0, 1, 2]\n",
      "range index: -2\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "Input: [1, 14]\n",
      "Output: 13\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 14]\n",
      "context_words1 : [[1, 14]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 1\n",
      "Word at index 1 : '1'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: -1\n",
      "end: 4\n",
      "Choosing context word based on condition:\n",
      "[-1, 0, 1, 2, 3]\n",
      "range index: -1\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '13'\n",
      "Word at range index at 0 is '13'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "Input: [13, 14, 3]\n",
      "Output: 1\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [13, 14, 3]\n",
      "context_words1 : [[13, 14, 3]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 2\n",
      "Word at index 2 : '14'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 0\n",
      "end: 5\n",
      "Choosing context word based on condition:\n",
      "[0, 1, 2, 3, 4]\n",
      "range index: 0\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '13'\n",
      "Word at range index at 0 is '13'\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "Input: [13, 1, 3, 4]\n",
      "Output: 14\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [13, 1, 3, 4]\n",
      "context_words1 : [[13, 1, 3, 4]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 3\n",
      "Word at index 3 : '3'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 1\n",
      "end: 6\n",
      "Choosing context word based on condition:\n",
      "[1, 2, 3, 4, 5]\n",
      "range index: 1\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '1'\n",
      "Word at range index at 1 is '1'\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '15'\n",
      "Word at range index at 5 is '15'\n",
      "Input: [1, 14, 4, 15]\n",
      "Output: 3\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [1, 14, 4, 15]\n",
      "context_words1 : [[1, 14, 4, 15]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 4\n",
      "Word at index 4 : '4'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 2\n",
      "end: 7\n",
      "Choosing context word based on condition:\n",
      "[2, 3, 4, 5, 6]\n",
      "range index: 2\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '14'\n",
      "Word at range index at 2 is '14'\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '15'\n",
      "Word at range index at 5 is '15'\n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [14, 3, 15]\n",
      "Output: 4\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [14, 3, 15]\n",
      "context_words1 : [[14, 3, 15]]\n",
      "*****************************\n",
      "sentence_length: 6\n",
      "WindowSize: 2\n",
      "index: 5\n",
      "Word at index 5 : '15'\n",
      "start = index - window_size\n",
      "end = index + window_size + 1\n",
      "start: 3\n",
      "end: 8\n",
      "Choosing context word based on condition:\n",
      "[3, 4, 5, 6, 7]\n",
      "range index: 3\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '3'\n",
      "Word at range index at 3 is '3'\n",
      "range index: 4\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "Statisfied the condition 'i != index'\n",
      "Selected Word: '4'\n",
      "Word at range index at 4 is '4'\n",
      "range index: 5\n",
      "Statisfied the condition '0 <= i' \n",
      "Statisfied the condition 'i < sentence_length' \n",
      "range index: 6\n",
      "Statisfied the condition '0 <= i' \n",
      "range index: 7\n",
      "Statisfied the condition '0 <= i' \n",
      "Input: [3, 4]\n",
      "Output: 15\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "context_words : [3, 4]\n",
      "context_words1 : [[3, 4]]\n",
      "*****************************\n",
      "###############################################################################\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "\n",
    "for words in wids:\n",
    "    print(words)\n",
    "    sentence_length = len(words)\n",
    "    for index,word in enumerate(words):\n",
    "        print(\"sentence_length:\",sentence_length)\n",
    "        print(\"WindowSize:\",window_size)\n",
    "        print(\"index:\",index)\n",
    "        print(\"Word at index {0} : '{1}'\".format(index,word))\n",
    "        \n",
    "        context_words =[]\n",
    "        context_words1 = []\n",
    "       \n",
    "        print(\"start = index - window_size\")\n",
    "        print(\"end = index + window_size + 1\")\n",
    "        \n",
    "        start = index - window_size\n",
    "        end = index + window_size + 1\n",
    "        \n",
    "\n",
    "        print(\"start:\",start)\n",
    "        print(\"end:\",end)\n",
    "       \n",
    "        print(\"Choosing context word based on condition:\")\n",
    "        \n",
    "        print([x for x in range(start, end)])\n",
    "        for i in range(start, end):\n",
    "            \n",
    "            print(\"range index:\",i)\n",
    "            \n",
    "            if 0 <= i:\n",
    "                print(\"Statisfied the condition '0 <= i' \")\n",
    "                \n",
    "                if i < sentence_length:\n",
    "                    print(\"Statisfied the condition 'i < sentence_length' \")\n",
    "                    \n",
    "                    if i != index:\n",
    "                        print(\"Statisfied the condition 'i != index'\")\n",
    "                        print(\"Selected Word: '{0}'\".format(words[i]))\n",
    "            \n",
    "            if  0 <= i < sentence_length and i != index:\n",
    "                print(\"Word at range index at {0} is '{1}'\".format(i,words[i]))\n",
    "                context_words.append(words[i])\n",
    "                \n",
    "        \n",
    "        print(\"Input:\",context_words)   \n",
    "        print(\"Output:\",words[index])\n",
    "        \n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "    \n",
    "        context_words1.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"context_words :\",context_words)\n",
    "        print(\"context_words1 :\",context_words1)\n",
    "        print(\"*****************************\")\n",
    "    print(\"###############################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "\n",
    "window_size = 2\n",
    "\n",
    "def generator():\n",
    "    for words in wids:\n",
    "        sentence_length = len(words)\n",
    "        for index,word in enumerate(words):\n",
    "            context_words =[]\n",
    "            label_word = []\n",
    "\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\n",
    "\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context_words, maxlen=7)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: [[0 0 0 0 0 1 6]]  output: [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 5 6 7]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 5 1 7 2]]  output: [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 1 6 2]]  output: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 6 7]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 8 9]]  output: [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0  2  9 10]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  2  8 10 11]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  8  9 11  3]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  9 10  3  4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0 10 11  4 12]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0 11  3 12]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0  0  1 14]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "input: [[ 0  0  0  0 13 14  3]]  output: [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0 13  1  3  4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
      "input: [[ 0  0  0  1 14  4 15]]  output: [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[ 0  0  0  0 14  3 15]]  output: [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "input: [[0 0 0 0 0 3 4]]  output: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for x ,y in generator():\n",
    "    print(\"input: {0}  output: {1}\".format(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(heaven (7), beginning (5)) -> 1\n",
      "(heaven (7), created (6)) -> 1\n",
      "(god (1), beginning (5)) -> 1\n",
      "(heaven (7), earth (2)) -> 1\n",
      "(beginning (5), heaven (7)) -> 1\n",
      "(beginning (5), god (1)) -> 0\n",
      "(beginning (5), void (10)) -> 0\n",
      "(beginning (5), god (1)) -> 1\n",
      "(beginning (5), earth (2)) -> 1\n",
      "(god (1), created (6)) -> 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "# generate skip-grams\n",
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "\n",
    "# view sample skip-grams\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 16\n",
      "Vocabulary Sample: [('god', 1), ('earth', 2), ('upon', 3), ('face', 4), ('beginning', 5), ('created', 6), ('heaven', 7), ('without', 8), ('form', 9), ('void', 10)]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "vocab_size = len(word2id) + 1 \n",
    "embed_size = 100\n",
    "\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Vocabulary Sample:', list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
