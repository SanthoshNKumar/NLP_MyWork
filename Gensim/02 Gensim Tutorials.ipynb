{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "\n",
    "t_corpus = [\n",
    "               \"A survey of user opinion of computer system response time\",\n",
    "               \"Relation of user perceived response time to error measurement\",\n",
    "               \"The generation of random binary unordered trees\",\n",
    "               \"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "          ]\n",
    "\n",
    "# No of documents : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words\n",
    "stoplist = set('for a of the and to in'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'and', 'for', 'in', 'of', 'the', 'to'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and Split \n",
    "processed_corpus = [[word for word in document.lower().split() if word not in stoplist] for document in t_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(25 unique tokens: ['computer', 'opinion', 'response', 'survey', 'system']...)\n"
     ]
    }
   ],
   "source": [
    "# Converting corpus into list of vectors\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'opinion': 1, 'response': 2, 'survey': 3, 'system': 4, 'time': 5, 'user': 6, 'error': 7, 'measurement': 8, 'perceived': 9, 'relation': 10, 'binary': 11, 'generation': 12, 'random': 13, 'trees': 14, 'unordered': 15, 'graph': 16, 'intersection': 17, 'paths': 18, 'iv': 19, 'minors': 20, 'ordering': 21, 'quasi': 22, 'well': 23, 'widths': 24}\n"
     ]
    }
   ],
   "source": [
    "# Word to id\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(2, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(14, 1), (16, 1), (17, 1), (18, 1)], [(14, 1), (16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# bag-of-word representation for a document \n",
    "\n",
    "BoW_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "\n",
    "print(BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], allow_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, 0.4869354917707381), (16, 0.8734379353188121)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(BoW_corpus)\n",
    "\n",
    "words = \"trees graph\".lower().split()\n",
    "\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "# Show the word to id map\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you get new documents in the future.it is also possible to update an existing dictionary to include the new words.\n",
    "\n",
    "documents_2 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]\n",
    "\n",
    "dictionary.add_documents(texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32, 'graph': 33, 'in': 34, 'intersection': 35, 'paths': 36, 'trees': 37, 'Graph': 38, 'IV': 39, 'Widths': 40, 'and': 41, 'minors': 42, 'ordering': 43, 'quasi': 44, 'well': 45, 'A': 46, 'survey': 47}\n"
     ]
    }
   ],
   "source": [
    "# Show the word to id map\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saudis'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saudis'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bag of words corpus in gensim\n",
    "\n",
    "#  Bag of Words : It contains the word id and its frequeny in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)], [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(7, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(23, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(x, allow_update=True) for x in texts]\n",
    "\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.doc2bow(['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], allow_update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Dictionary and Corpus\n",
    "\n",
    "dictionary.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)  # save corpus to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 2.0), (8, 1.0)]\n",
      "[(9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0), (14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0)]\n",
      "[(7, 2.0), (18, 1.0), (19, 1.0), (20, 1.0), (21, 1.0), (22, 1.0), (23, 1.0), (24, 1.0), (25, 1.0)]\n",
      "[(23, 2.0), (26, 1.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 1.0), (31, 1.0), (32, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "\n",
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bigrams and trigrams using Phraser models?\n",
    "\n",
    "documents_3 = [\"The intersection graph of paths in trees\",\n",
    "               \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "               \"Graph minors A survey\"]\n",
    "\n",
    "texts_3 = [[text for text in doc.split()] for doc in documents_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)], [(7, 1), (11, 1), (15, 1), (16, 1)]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dct = corpora.Dictionary(texts_3)\n",
    "\n",
    "corpus = [dct.doc2bow(line) for line in texts_3]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time']\n"
     ]
    }
   ],
   "source": [
    "bigram = gensim.models.phrases.Phrases(\"The intersection graph of paths in trees\", min_count=3, threshold=10)\n",
    "\n",
    "print(bigram['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Saudis', 'are', 'preparing', 'a', 'report', 'that', 'will', 'acknowledge', 'that'], ['Saudi', 'journalist', 'Jamal', \"Khashoggi's\", 'death', 'was', 'the', 'result', 'of', 'an'], ['interrogation', 'that', 'went', 'wrong,', 'one', 'that', 'was', 'intended', 'to', 'lead'], ['to', 'his', 'abduction', 'from', 'Turkey,', 'according', 'to', 'two', 'sources.']]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the Dataset\n",
    "\n",
    "import gensim\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# List of Tokens\n",
    "tokenized  = [doc.split() for doc in documents]\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "# Create a Dictionary\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "# storing the extracted tokens into the dictionary\n",
    "my_dictionary = corpora.Dictionary(tokenized)\n",
    "\n",
    "print(my_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1)], [(9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(7, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)], [(23, 2), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# convertig to a bag of word corpus\n",
    "\n",
    "BOW_corpus = [my_dictionary.doc2bow(doc, allow_update = True) for doc in tokenized]\n",
    "\n",
    "print(BOW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Saudis', 1], ['The', 1], ['a', 1], ['acknowledge', 1], ['a', 1], ['acknowledge', 1], ['are', 2], ['Saudis', 2], ['acknowledge', 3], ['preparing', 2], ['report', 1], ['that', 2], ['will', 1]]\n"
     ]
    }
   ],
   "source": [
    "#  Create a TFIDF matrix in Gensim\n",
    "import numpy as np\n",
    "\n",
    "word_weight =[]\n",
    "for doc in BoW_corpus:\n",
    "    for id, freq in doc:\n",
    "        word_weight.append([my_dictionary[id], freq])\n",
    "\n",
    "print(word_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Saudis', 0.403], ['The', 0.805], ['a', 0.403], ['acknowledge', 0.167], ['a', 0.241], ['acknowledge', 0.1], ['are', 0.965], ['Saudis', 0.296], ['acknowledge', 0.184], ['preparing', 0.593], ['report', 0.296], ['that', 0.593], ['will', 0.296]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# create TF-IDF model\n",
    "tfIdf = models.TfidfModel(BoW_corpus, smartirs ='ntc')\n",
    "\n",
    "# TF-IDF Word Weight\n",
    "weight_tfidf =[]\n",
    "for doc in tfIdf[BoW_corpus]:\n",
    "    for id, freq in doc:\n",
    "        weight_tfidf.append([my_dictionary[id], np.around(freq, decimals = 3)])\n",
    "        \n",
    "print(weight_tfidf)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from multiprocessing import cpu_count\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# load the text8 dataset\n",
    "dataset = api.load(\"text8\")\n",
    "\n",
    "data = []\n",
    "for word in dataset:\n",
    "    data.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1701"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Implementation and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Word2Vec \n",
    "\n",
    "model = Word2Vec(sentences=common_texts, max_vocab_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"word2vec.model\")\n",
    "# model = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you save the model you can continue training it later:\n",
    "model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human': <gensim.models.keyedvectors.Vocab at 0x22f8711bc08>,\n",
       " 'interface': <gensim.models.keyedvectors.Vocab at 0x22fcf783648>,\n",
       " 'computer': <gensim.models.keyedvectors.Vocab at 0x22fcf783548>,\n",
       " 'survey': <gensim.models.keyedvectors.Vocab at 0x22fcf783708>,\n",
       " 'user': <gensim.models.keyedvectors.Vocab at 0x22fcf7830c8>,\n",
       " 'system': <gensim.models.keyedvectors.Vocab at 0x22fcf783848>,\n",
       " 'response': <gensim.models.keyedvectors.Vocab at 0x22fcf783748>,\n",
       " 'time': <gensim.models.keyedvectors.Vocab at 0x22fcf7836c8>,\n",
       " 'eps': <gensim.models.keyedvectors.Vocab at 0x22fcf783788>,\n",
       " 'trees': <gensim.models.keyedvectors.Vocab at 0x22fcf7837c8>,\n",
       " 'graph': <gensim.models.keyedvectors.Vocab at 0x22fcf783688>,\n",
       " 'minors': <gensim.models.keyedvectors.Vocab at 0x22fcf783808>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.2501707e-03, -2.7321007e-03, -7.6862931e-04,  9.1923954e-05,\n",
       "       -2.8173858e-03,  7.0831645e-04, -2.3521099e-03, -3.1246059e-03,\n",
       "        3.3323485e-03,  3.1188789e-03, -4.4940435e-04, -4.7564129e-03,\n",
       "        1.8066826e-03, -2.6453191e-03, -3.0376876e-03, -4.4731266e-04,\n",
       "       -4.3843221e-03, -4.5795701e-03,  4.5116562e-03, -3.9353822e-03,\n",
       "       -4.2268410e-03,  3.0567494e-04,  2.5643543e-03,  4.3433835e-03,\n",
       "        3.7493519e-03, -1.6155867e-03,  3.4571714e-03,  4.6889563e-03,\n",
       "        3.3529974e-03,  2.4794789e-03, -1.0330174e-03, -3.0484376e-03,\n",
       "        3.4154563e-03, -1.7702365e-03,  8.1581093e-04, -3.7170528e-03,\n",
       "        3.4322981e-03, -3.2531032e-03,  3.4183101e-03, -3.3949781e-03,\n",
       "        2.5810129e-03,  2.7383575e-03,  3.1628022e-03,  1.5606054e-03,\n",
       "       -3.0438297e-03, -4.3455143e-03,  2.7804571e-04, -4.1938298e-03,\n",
       "       -3.1540147e-03,  4.8645400e-03, -1.1250745e-03,  2.7935701e-05,\n",
       "       -2.4871968e-03, -7.7383523e-04,  1.0868582e-03, -1.8671358e-03,\n",
       "       -2.9748988e-03, -3.4357116e-03,  4.9353028e-03,  1.2869646e-03,\n",
       "        1.6165331e-03, -4.1731345e-03,  8.9192262e-04,  3.5846950e-03,\n",
       "       -2.9121998e-03,  4.0856129e-03, -3.4162155e-03, -1.5683277e-04,\n",
       "        4.9309013e-03,  1.1907629e-03,  4.6932450e-04, -4.1040154e-03,\n",
       "       -4.4785393e-03,  4.4071241e-03,  8.8755900e-05, -1.7439027e-03,\n",
       "        3.3820209e-03, -1.2858477e-03, -3.9682533e-03,  6.8298366e-04,\n",
       "       -2.6079165e-03,  6.7659642e-04,  1.1874933e-03, -3.2344682e-03,\n",
       "       -2.5714380e-03, -1.9134172e-03, -3.0633558e-03, -1.8695097e-03,\n",
       "       -1.9922715e-03,  2.9128981e-03, -1.6806765e-03, -4.8763151e-03,\n",
       "       -2.3510559e-03, -6.6042715e-04, -4.9328883e-03, -4.1825562e-03,\n",
       "        3.1411063e-03, -1.0287215e-03,  1.6688198e-03, -1.8319143e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bulding the vector : Numpy array for the word \n",
    "\n",
    "model.wv['computer']  # get numpy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Tv' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-58775e104ee3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Tv\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# word 'Tv' not in vocabulary . it will through error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\insakum46\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\insakum46\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\insakum46\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'Tv' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.wv[\"Tv\"]\n",
    "\n",
    "# word 'Tv' not in vocabulary . it will through error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.2501707e-03, -2.7321007e-03, -7.6862931e-04,  9.1923954e-05,\n",
       "        -2.8173858e-03,  7.0831645e-04, -2.3521099e-03, -3.1246059e-03,\n",
       "         3.3323485e-03,  3.1188789e-03, -4.4940435e-04, -4.7564129e-03,\n",
       "         1.8066826e-03, -2.6453191e-03, -3.0376876e-03, -4.4731266e-04,\n",
       "        -4.3843221e-03, -4.5795701e-03,  4.5116562e-03, -3.9353822e-03,\n",
       "        -4.2268410e-03,  3.0567494e-04,  2.5643543e-03,  4.3433835e-03,\n",
       "         3.7493519e-03, -1.6155867e-03,  3.4571714e-03,  4.6889563e-03,\n",
       "         3.3529974e-03,  2.4794789e-03, -1.0330174e-03, -3.0484376e-03,\n",
       "         3.4154563e-03, -1.7702365e-03,  8.1581093e-04, -3.7170528e-03,\n",
       "         3.4322981e-03, -3.2531032e-03,  3.4183101e-03, -3.3949781e-03,\n",
       "         2.5810129e-03,  2.7383575e-03,  3.1628022e-03,  1.5606054e-03,\n",
       "        -3.0438297e-03, -4.3455143e-03,  2.7804571e-04, -4.1938298e-03,\n",
       "        -3.1540147e-03,  4.8645400e-03, -1.1250745e-03,  2.7935701e-05,\n",
       "        -2.4871968e-03, -7.7383523e-04,  1.0868582e-03, -1.8671358e-03,\n",
       "        -2.9748988e-03, -3.4357116e-03,  4.9353028e-03,  1.2869646e-03,\n",
       "         1.6165331e-03, -4.1731345e-03,  8.9192262e-04,  3.5846950e-03,\n",
       "        -2.9121998e-03,  4.0856129e-03, -3.4162155e-03, -1.5683277e-04,\n",
       "         4.9309013e-03,  1.1907629e-03,  4.6932450e-04, -4.1040154e-03,\n",
       "        -4.4785393e-03,  4.4071241e-03,  8.8755900e-05, -1.7439027e-03,\n",
       "         3.3820209e-03, -1.2858477e-03, -3.9682533e-03,  6.8298366e-04,\n",
       "        -2.6079165e-03,  6.7659642e-04,  1.1874933e-03, -3.2344682e-03,\n",
       "        -2.5714380e-03, -1.9134172e-03, -3.0633558e-03, -1.8695097e-03,\n",
       "        -1.9922715e-03,  2.9128981e-03, -1.6806765e-03, -4.8763151e-03,\n",
       "        -2.3510559e-03, -6.6042715e-04, -4.9328883e-03, -4.1825562e-03,\n",
       "         3.1411063e-03, -1.0287215e-03,  1.6688198e-03, -1.8319143e-03],\n",
       "       [-3.5884501e-03, -2.2577411e-03, -1.0955224e-03,  2.3025132e-03,\n",
       "         3.4998900e-03,  3.8459767e-03, -1.7707036e-03,  4.2799111e-03,\n",
       "        -3.0256165e-03, -2.4459211e-04, -4.0092398e-03,  4.0133370e-04,\n",
       "        -7.0333382e-04,  4.1939403e-04, -3.9853943e-03,  1.0803459e-03,\n",
       "        -3.3456940e-04, -1.7003916e-04, -1.9163591e-03, -8.6694263e-04,\n",
       "         3.6297122e-04,  4.7248565e-03,  1.9441175e-03, -4.0533566e-03,\n",
       "        -2.3086567e-03, -3.1183296e-04, -1.3112029e-03, -1.8376895e-03,\n",
       "        -2.3053556e-03, -3.3832188e-03,  6.0145274e-05, -2.8493896e-03,\n",
       "        -4.3948125e-03,  4.1788188e-04,  4.8084590e-03,  9.1255392e-04,\n",
       "         1.1191211e-03,  1.4430456e-03,  6.3559163e-04, -4.1106590e-03,\n",
       "        -2.7542417e-03, -7.6078606e-04,  6.2844675e-04,  2.8846294e-03,\n",
       "        -4.6360982e-03,  7.0835504e-04,  4.0130243e-03,  6.2977843e-04,\n",
       "        -4.9430747e-03,  4.2233304e-03, -3.6734564e-03,  2.0278667e-03,\n",
       "        -3.2293631e-03,  3.2191507e-03, -6.9797167e-04, -4.5071240e-03,\n",
       "        -9.9852635e-04,  1.8675439e-03,  2.1059583e-03, -2.5723341e-03,\n",
       "        -2.6057228e-03,  2.4863197e-03, -4.9939332e-05,  3.5138882e-03,\n",
       "         2.5575245e-03,  8.2825421e-04,  1.2717409e-04, -4.3382472e-03,\n",
       "         8.4678922e-04, -2.8739697e-03, -1.8460354e-03,  3.2336372e-03,\n",
       "         3.3427053e-04, -4.2255684e-03, -3.7817231e-03,  2.7283258e-03,\n",
       "        -1.1718811e-03, -3.9867018e-03,  3.0836463e-03, -4.9002203e-03,\n",
       "        -4.1281986e-03, -2.4019931e-03,  1.7767083e-03, -2.3572226e-03,\n",
       "        -1.6941308e-03, -7.6334281e-06,  3.9117578e-03,  3.3076336e-03,\n",
       "        -1.0548687e-03,  7.1706710e-04,  1.2099457e-03, -2.1391341e-03,\n",
       "         4.0784762e-03, -1.8444546e-03,  1.6437165e-03, -4.1926689e-03,\n",
       "        -4.7035264e-03, -1.7239174e-03, -4.4327695e-03,  2.0953664e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['computer','human']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eps', 0.22728891670703888),\n",
       " ('system', 0.14085102081298828),\n",
       " ('graph', 0.06629200279712677),\n",
       " ('survey', 0.06482338905334473),\n",
       " ('trees', 0.030946407467126846),\n",
       " ('response', 0.008782591670751572),\n",
       " ('minors', 0.002620246261358261),\n",
       " ('interface', -0.023094624280929565),\n",
       " ('user', -0.04825381562113762),\n",
       " ('human', -0.04944903403520584)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get other similar words\n",
    "\n",
    "sims = model.wv.most_similar('computer', topn=10)  \n",
    "\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
